tune_hyperparameters: true  # true: tune hyperparameters, false: use stored hyperparameters

missing_values: 3  # 0: don't handle missing values, 1: drop rows with missing values, 2: impute with mode(cat) and mean(num), 3: impute with median(num) and mode(cat)
encode_cat: 1  # 0: no encoding, 1: one-hot, 2: WoE encoding
standardize: 2  # 0: no standardization, 1: standardnormal distribution, 2: min-max scaling

methods_lgd:
  ab: false      # AdaBoost Regression (AB)
  ann: false     # Artificial Neural Networks (ANN)
  cb: false      # CatBoost Regression (CB)
  dt: true      # Decision Tree Regression (DT)
  en: false      # Elastic Net Regression (EN)
  knn: false     # K-Nearest Neighbors Regression (KNN)
  lgbm: false    # LightGBM Regression (LGBM)
  lr: false      # Linear Regression (LR)
  rf: false      # Random Forest Regression (RF)
  svr: false     # Support Vector Regression (SVR)
  tabnet: true  # TabNet Regression (TabNet)
  tabpfn: false  # TabPFN Regression (TabPFN)
  xgb: false     # Extreme Gradient Boosting Regression (XGB)


methods_pd:
  ab: false    # AdaBoost (AB)
  ann: false   # Artificial Neural Networks (ANN)
  bnb: false   # Bernoulli Naive Bayes (BNB)
  cb: false    # CatBoost (CB)
  dt: true    # Decision Tree (DT)
  gnb: false   # Gaussian Naive Bayes (GNB)
  knn: false   # K-Nearest Neighbors (KNN)
  lda: false   # Linear Discriminant Analysis (LDA)
  lgbm: false  # LightGBM (LGBM)
  lr: false    # Logistic Regression (LR)
  rf: false    # Random Forest (RF)
  svm: false   # Support Vector Machine (SVM)
  tabnet: true   # TabNet
  tabpfn: false   # TabPFN foundation model
  xgb: false   # Extreme Gradient Boosting (XGBoost)

hyperparameters_lgd:
  ab:
    n_estimators: [50, 100]
    learning_rate: [0.01, 0.1]
  ann:
    num_hidden_layers: [1]
    hidden_layer_size: [32,64]
    activation: ['relu']
    dropout_rate: [0,0.1]
    epochs: [50]
    batch_size: [256]
    learning_rate: [0.001]
    weight_decay: [0.01]
    early_stopping_delay: [5]
  cb:
    iterations: [100, 200]
    depth: [4, 6]
    verbose: [False]
  dt:
    criterion: ['absolute_error']
    max_depth: [10]
  en:
    alpha: [0.1, 0.5]
    l1_ratio: [0.1, 0.5]
  gb:
    n_estimators: [50, 100]
    learning_rate: [0.01, 0.1]
  knn:
    n_neighbors: [3, 5]
  lgbm:
    n_estimators: [50]
    learning_rate: [0.01, 0.1]
  lr:
    fit_intercept: [True] # to provide at least one hyperpara
  rf:
    n_estimators: [100, 200]
    max_depth: [3, 5]
  svr:
    C: [0.1, 1]
    kernel: ['linear', 'rbf']
  tabnet:
    scheduler_params: [ { "step_size": 10, "gamma": 0.9 },
                        { "step_size": 20, "gamma": 0.9 } ]
  #  n_d: [ 8, 16 ]
    n_shared: [ 1,2,3 ]
    n_steps: [ 3,6,9 ]
  tabpfn:
    device: ['auto']
  xgb:
    n_estimators: [50, 100]
    max_depth: [3, 5]
    learning_rate: [0.01, 0.1]

hyperparameters_pd:  # For each method, the hyperparameters to tune
  ab:
    n_estimators: [50, 100]
    learning_rate: [0.01, 0.1]
  ann:
    num_hidden_layers: [1]
    hidden_layer_size: [32,64]
    activation: ['relu']
    dropout_rate: [0,0.1]
    epochs: [50]
    batch_size: [256]
    learning_rate: [0.001]
    weight_decay: [0.01]
    early_stopping_delay: [5]
  bnb:
    alpha: [0.1, 0.5]
  cb:
    iterations: [100, 200]
    depth: [4, 6]
  dt:
    criterion: ['gini']
    max_depth: [10]
  gnb:
    var_smoothing: [0.0001, 0.001]
  knn:
    n_neighbors: [3, 5]
  lda:
    solver: [ 'svd', 'lsqr' ]
  lgbm:
    n_estimators: [50]
    learning_rate: [0.01, 0.1]
  lr:
    penalty: ['l2']
    C: [0.1, 1]
  rf:
    n_estimators: [100, 200]
    max_depth: [3, 5]
  svm:
    C: [0.1, 1]
    kernel: ['linear', 'rbf']
  tabnet:
    scheduler_params: [{"step_size":10, "gamma":0.9},
                       {"step_size":20, "gamma":0.9}]
    n_d: [8, 16]
    n_shared: [1,2,3]
    n_steps: [3,6,9]
  tabpfn:
    device: ['auto']
  xgb:
    n_estimators: [50, 100]
    max_depth: [3, 5]
    learning_rate: [0.01, 0.1]

