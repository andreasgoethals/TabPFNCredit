#tune_hyperparameters: false  # true: tune hyperparameters, false: use stored hyperparameters

missing_values: 1  # 0: don't handle missing values, 1: drop rows with missing values, 2: impute with mode(cat) and mean(num), 3: impute with median(num) and mode(cat)
encode_cat: 1  # 0: no encoding, 1: one-hot, 2: WoE encoding
standardize: 2  # 0: no standardization, 1: standardnormal distribution, 2: min-max scaling

methods_lgd:
  ab: true      # AdaBoost Regression (AB)
  ann: true     # Artificial Neural Networks (ANN)
  cb: true      # CatBoost Regression (CB)
  dt: true      # Decision Tree Regression (DT)
  en: true      # Elastic Net Regression (EN)
  knn: true     # K-Nearest Neighbors Regression (KNN)
  lgbm: true    # LightGBM Regression (LGBM)
  lr: true      # Linear Regression (LR)
  rf: true      # Random Forest Regression (RF)
  svr: true     # Support Vector Regression (SVR)
  tabnet: true  # TabNet Regression (TabNet)
  tabpfn: true  # TabPFN Regression (TabPFN)
  xgb: true     # Extreme Gradient Boosting Regression (XGB)


methods_pd:
  ab: false    # AdaBoost (AB)
  ann: false   # Artificial Neural Networks (ANN)
  bnb: false   # Bernoulli Naive Bayes (BNB)
  cb: false    # CatBoost (CB)
  dt: true    # Decision Tree (DT)
  gnb: false   # Gaussian Naive Bayes (GNB)
  knn: false   # K-Nearest Neighbors (KNN)
  lda: false   # Linear Discriminant Analysis (LDA)
  lgbm: false  # LightGBM (LGBM)
  lr: true    # Logistic Regression (LR)
  rf: false # Random Forest (RF)
  svm: false   # Support Vector Machine (SVM)
  tabnet: false   # TabNet
  tabpfn: true   # TabPFN foundation model
  xgb: false   # Extreme Gradient Boosting (XGBoost)

hyperparameters_lgd:
  ab:
    n_estimators: [50, 100]
    learning_rate: [0.01, 0.1]
  ann:
    num_hidden_layers: [1]
    hidden_layer_size: [32,64]
    activation: ['relu']
    dropout_rate: [0,0.1]
    epochs: [50]
    batch_size: [256]
    learning_rate: [0.001]
    weight_decay: [0.01]
    early_stopping_delay: [5]
  cb:
    iterations: [50, 100] #, 250]
    #depth: [4, 6, 10]
    #learning_rate: [0.01, 0.1]
    verbose: [False]
  dt:
    criterion: ['absolute_error']
    max_depth: [10]
  en:
    alpha: [0.1, 0.5]
    l1_ratio: [0.1, 0.5]
  gb:
    n_estimators: [50, 100]
    learning_rate: [0.01, 0.1]
  knn:
    n_neighbors: [3, 5]
  lgbm:
    n_estimators: [50, 100, 250]
    max_depth: [3, 6, 10]
    learning_rate: [0.01, 0.1]
  lr:
    fit_intercept: [True, False] # to provide at least one hyperpara
  rf:
    #n_estimators: [100, 200, 300]
    max_depth: [10, 50]
    #min_samples_split: [2]
    #min_samples_leaf: [1, 2]
    #max_features: ["sqrt", "log2", None]
  svr:
    C: [0.1, 1]
    kernel: ['linear', 'rbf']
  tabnet:
    scheduler_params: [ { "step_size": 10, "gamma": 0.9 },
                        { "step_size": 20, "gamma": 0.9 } ]
  #  n_d: [ 8, 16 ]
    n_shared: [ 1,2,3 ]
    n_steps: [ 3,6,9 ]
  tabpfn:
    device: ['auto']
    ignore_pretraining_limits: [false]
  xgb:
    n_estimators: [50, 100]
    #max_depth: [3, 6, 10]
    #learning_rate: [0.01, 0.1]

hyperparameters_pd:  # For each method, the hyperparameters to tune
  ab:
    n_estimators: [50, 100]
    learning_rate: [0.01, 0.1]
  ann:
    num_hidden_layers: [1]
    hidden_layer_size: [32,64]
    activation: ['relu']
    dropout_rate: [0,0.1]
    epochs: [50]
    batch_size: [256]
    learning_rate: [0.001]
    weight_decay: [0.01]
    early_stopping_delay: [5]
  bnb:
    alpha: [0.1, 0.5]
  cb:
    iterations: [50, 100, 250]
    depth: [4, 6, 10]
    learning_rate: [0.01, 0.1]
  dt:
    criterion: ['gini']
    max_depth: [10]
  gnb:
    var_smoothing: [0.0001, 0.001]
  knn:
    n_neighbors: [3, 5]
  lda:
    solver: [ 'svd', 'lsqr' ]
  lgbm:
    n_estimators: [50, 100, 250]
    max_depth: [3, 6, 10]
    learning_rate: [0.01, 0.1]
    #subsample: [0.5, 0.7, 1]
    #colsample_bytree: [0.6, 0.8, 1.0]
  lr:
    penalty: ['l2']
    C: [0.1, 1]
  rf:
    n_estimators: [100, 200, 300]
    max_depth: [50, 100]
    min_samples_split: [2]
    #min_samples_leaf: [1, 2]
    max_features: ["sqrt", "log2", None]
  svm:
    C: [0.1, 1]
    kernel: ['linear', 'rbf']
  tabnet:
    scheduler_params: [{"step_size":10, "gamma":0.9},
                       {"step_size":20, "gamma":0.9}]
    n_d: [8, 16]
    n_shared: [1,2,3]
    n_steps: [3,6,9]
  tabpfn:
    device: ['auto']
    ignore_pretraining_limits: [false]
  xgb:
    n_estimators: [50, 100, 250]
    max_depth: [3, 6, 10]
    learning_rate: [0.01, 0.1]
    #subsample: [0.5, 0.7, 1]
    #colsample_bytree: [0.6, 0.8, 1.0]

#default_hyperparameters_pd:  # For each method, the hyperparameters to tune
#  ab:
#    n_estimators: 50
#    learning_rate: 0.1
#  ann:
#    num_hidden_layers: 1
#    hidden_layer_size: 32,64
#    activation: 'relu'
#    dropout_rate: 0.1
#    epochs: 50
#    batch_size: 256
#    learning_rate: 0.001
#    weight_decay: 0.01
#    early_stopping_delay: 5
#  bnb:
#    alpha: 0.1
#  cb:
#    iterations: 50
#    depth: 4
#    learning_rate: 0.01
#  dt:
#    criterion: 'gini'
#    max_depth: 10
#  gnb:
#    var_smoothing: 0.0001
#  knn:
#    n_neighbors: 3
#  lda:
#    solver: 'svd'
#  lgbm:
#    n_estimators: 50
#    max_depth: 3
#    learning_rate: 0.01
#    #subsample: [0.5, 0.7, 1]
#    #colsample_bytree: [0.6, 0.8, 1.0]
#  lr:
#    penalty: 'l2'
#    C: 0.1
#  rf:
#    n_estimators: 100
#    max_depth: None
#    min_samples_split: 2
#    #min_samples_leaf: [1, 2]
#    max_features: "sqrt"
#  svm:
#    C: 0.1
#    kernel: 'linear'
#  tabnet:
#    scheduler_params: [{"step_size":10, "gamma":0.9},
#                       {"step_size":20, "gamma":0.9}]
#    n_d: 8
#    n_shared: 1
#    n_steps: 3
#  tabpfn:
#    device: ['auto']
#    ignore_pretraining_limits: [false]
#  xgb:
#    n_estimators: [50, 100, 250]
#    max_depth: [3, 6, 10]
#    learning_rate: [0.01, 0.1]
#    #subsample: [0.5, 0.7, 1]
#    #colsample_bytree: [0.6, 0.8, 1.0]
