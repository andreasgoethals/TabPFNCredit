tune_hyperparameters: true  # true: tune hyperparameters, false: use base models
tuning_method: 'optuna'      # optuna | grid | none: use stored hyperparameters
n_trials: 20

tuning_params:
  pd:
    ab:
      param_space:
        n_estimators:
          type: 'int'
          low: 50
          high: 100
          step: 10
        learning_rate:
          type: 'float'
          low: 0.01
          high: 0.1
    ann:
    bnb:
    cb:
    dt:
    gnb:
    knn:
    lda:
    lgbm:
    lr:
      param_space:
        C:
          type: 'float'
          low: 0.001
          high: 100.0
          log: true
        penalty:
          type: 'categorical'
          values: [ 'l1', 'l2' ]
      param_grid:
        C: [ 0.001, 0.01, 0.1, 1.0, 10.0, 100.0 ]
        penalty: [ 'l1', 'l2','elasticnet' ]
    rf:
      param_space:
        n_estimators:
          type: 'int'
          low: 10
          high: 100
        max_depth:
          type: 'int'
          low: 1
          high: 32
      param_grid:
        n_estimators: [ 10, 100, 200 ]
        max_depth: [ 1, 16, 32 ]
    svm:
    tabnet:
    tabpfn:
      param_space: {}
      param_grid: {}
    xgb:
    # added this for random_forest tuning hp tuning -> kind of double tuning strategy
    tabpfn_rf:
      param_space:
        n_estimators:
          type: 'int'
          low: 50
          high: 200
          step: 50
        max_depth:
          type: 'int'
          low: 3
          high: 10
        min_samples_split:
          type: 'int'
          low: 2
          high: 100
        min_samples_leaf:
          type: 'int'
          low: 1
          high: 10
        criterion:
          type: 'categorical'
          values: [ 'gini', 'entropy' ]
        max_features:
          type: 'categorical'
          values: [ 'sqrt', 'log2', null ]
      param_grid:
        n_estimators: [ 50, 100, 200 ]
        max_depth: [ 5, 10, null ]
        min_samples_split: [ 2, 100 ]
        min_samples_leaf: [ 1, 5 ]
        criterion: [ 'gini', 'entropy' ]
        max_features: [ 'sqrt', null ]
    tabpfn_hpo: # this has an internal tuner already so we will not use Optuna or Grid search on this one
      n_trials: 50
      metric: 'f1'
      random_state: 42