"""
parse_outputs.py
----------------
Generates an aggregated model ranking table from the meta results of all enabled datasets.

**Purpose**
----------
For the currently active task (either 'pd' or 'lgd') specified in `CONFIG_EXPERIMENT.yaml`,
this script will:
1. Identify all datasets marked as `true` in `CONFIG_DATA.yaml` for that task.
2. For each enabled dataset, load its `<dataset>_meta_results.csv` file from `outputs/<task>/<dataset>/`.
- These meta results should already be generated by combining individual run CSVs.
3. Read the list of enabled metrics for the task from `CONFIG_EVALUATION.yaml`.
4. Average each metric over all cross-validation folds (`split` values) per model.
5. Rank models for each metric:
- Higher-is-better for metrics in the `maximize` set (e.g., accuracy, f1, r2, aucroc, etc.).
- Lower-is-better for all others.
- Missing values remain as NaN in the rank table (not penalized).
6. Output a **wide-format CSV** where:
- Rows = models.
- Columns = `<metric>_<dataset>` rank values.
- Lower rank = better performance.
- File is saved to `outputs/analysis/<task>_ranking.csv`.

**Usage**
---------
1. In `config/CONFIG_DATA.yaml`, enable (`true`) the datasets you want to include for the active task.
2. In `config/CONFIG_EVALUATION.yaml`, enable (`true`) the metrics you want to rank.
3. In `config/CONFIG_EXPERIMENT.yaml`, set:
- `task`: 'pd' or 'lgd'
- `imbalance`: `true` or `false` (only relevant if filtering by imbalance setting in meta results)
4. Ensure each enabled dataset folder under `outputs/<task>/` contains its `<dataset>_meta_results.csv`.
5. Run this script from the repository root.
"""


from __future__ import annotations
from pathlib import Path
import pandas as pd
import yaml

# ---------- Paths ----------
HERE = Path(__file__).resolve().parent           # .../analysis
ROOT = HERE.parent                                # repo root
CONFIG_DIR = ROOT / "config"
OUTPUTS_DIR = ROOT / "outputs"
ANALYSIS_DIR = OUTPUTS_DIR / "analysis"

EVAL_CFG = CONFIG_DIR / "CONFIG_EVALUATION.yaml"
EXP_CFG = CONFIG_DIR / "CONFIG_EXPERIMENT.yaml"
DATA_CFG = CONFIG_DIR / "CONFIG_DATA.yaml"

# ---------- Helpers ----------
def load_yaml(p: Path) -> dict:
    with p.open("r") as f:
        return yaml.safe_load(f)

def get_active_task(exp_cfg: dict) -> str:
    task = str(exp_cfg.get("task", "pd")).strip().lower()
    if task not in {"pd", "lgd"}:
        raise ValueError(f"Unsupported task in CONFIG_EXPERIMENT.yaml: {task}")
    return task

def get_enabled_datasets(task: str, data_cfg: dict) -> list[str]:
    key = "dataset_pd" if task == "pd" else "dataset_lgd"
    section = data_cfg.get(key, {}) or {}
    enabled = [name for name, on in section.items() if on]
    return enabled

def get_enabled_metrics(task: str, eval_cfg: dict) -> list[str]:
    metrics_section = (eval_cfg.get("metrics") or {}).get(task, {}) or {}
    metrics = [m for m, on in metrics_section.items() if on]
    return metrics

def maximize_set_for(task: str, metrics: list[str]) -> set[str]:
    if task == "pd":
        # higher is better except brier
        default_max = {"accuracy", "f1", "precision", "recall", "aucroc", "aucpr", "h_measure"}
        return set(m for m in metrics if m in default_max)
    else:
        # lgd: higher r2; others lower
        return {"r2"} & set(metrics)

def expected_meta_path(task: str, dataset: str) -> Path:
    return OUTPUTS_DIR / task / dataset / f"{dataset}_meta_results.csv"

def read_meta(meta_path: Path) -> pd.DataFrame | None:
    if not meta_path.exists():
        return None
    df = pd.read_csv(meta_path)
    return df

def average_over_folds(df: pd.DataFrame, metrics: list[str]) -> pd.DataFrame:
    # group by model, average over splits
    present = [m for m in metrics if m in df.columns]
    if not present:
        return pd.DataFrame()
    out = df.groupby("model", as_index=True)[present].mean()
    return out

def rank_frame(df_mean: pd.DataFrame, metrics: list[str],maximize: set[str], tie_method: str = "min") -> pd.DataFrame:
    ranks = {}
    for m in metrics:
        if m not in df_mean.columns:
            continue
        ascending = (m not in maximize)  # ascending=True => smaller is better
        col = df_mean[m]
        # Rank only non-NaN values, keep NaN as NaN
        ranked = col.rank(ascending=ascending, method=tie_method, na_option='keep')
        ranks[m] = ranked
    if not ranks:
        return pd.DataFrame(index=df_mean.index)
    return pd.DataFrame(ranks, index=df_mean.index)


# ---------- Main ----------
def main() -> int:
    ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)

    eval_cfg = load_yaml(EVAL_CFG)
    exp_cfg = load_yaml(EXP_CFG)
    data_cfg = load_yaml(DATA_CFG)

    task = get_active_task(exp_cfg)
    datasets = get_enabled_datasets(task, data_cfg)
    metrics = get_enabled_metrics(task, eval_cfg)
    if not metrics:
        print(f"[warn] No metrics enabled for task '{task}'. Nothing to do.")
        return 0

    maximize = maximize_set_for(task, metrics)
    imbalance_flag = bool(exp_cfg.get("imbalance", False))

    all_rankings_wide: dict[str, pd.DataFrame] = {}

    for dataset in datasets:
        meta_path = expected_meta_path(task, dataset)
        df = read_meta(meta_path)
        if df is None:
            print(f"[skip] Missing meta file: {meta_path}")
            continue

        if "model" not in df.columns:
            print(f"[skip] 'model' column missing in {meta_path.name}")
            continue

        df_mean = average_over_folds(df, metrics)
        if df_mean.empty:
            print(f"[skip] No enabled metrics present in {meta_path.name}")
            continue

        df_ranks = rank_frame(df_mean, metrics, maximize, tie_method="min")
        if df_ranks.empty:
            print(f"[skip] Ranking produced empty frame for {dataset}")
            continue

        # rename columns to metric_dataset
        df_ranks.columns = [f"{m}_{dataset}" for m in df_ranks.columns]
        df_ranks.index.name = "model"
        all_rankings_wide[dataset] = df_ranks

    if not all_rankings_wide:
        print("[warn] No rankings produced.")
        return 0

    merged = pd.concat(all_rankings_wide.values(), axis=1, join="outer").reset_index()
    out_path = ANALYSIS_DIR / f"{task}_ranking.csv"
    merged.to_csv(out_path, index=False)
    print(f"[ok] Ranking saved to {out_path} (models={merged.shape[0]}, cols={merged.shape[1]})")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())
