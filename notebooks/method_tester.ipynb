{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94eb2461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DEBUGGING: TALENT method_runner Analysis\n",
      "================================================================================\n",
      "\n",
      "TEST 1: Single fold MLP + HPO (inspect results dict)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "Running mlp (deep) on 0009.german (PD)\n",
      "======================================================================\n",
      "using gpu: 0\n",
      "{'batch_size': 1024,\n",
      " 'cat_min_frequency': 0.0,\n",
      " 'cat_nan_policy': 'new',\n",
      " 'cat_policy': 'ordinal',\n",
      " 'config': {'general': {},\n",
      "            'model': {'d_layers': [384, 384], 'dropout': 0.1},\n",
      "            'training': {'lr': 0.0003, 'weight_decay': 1e-05}},\n",
      " 'dataset': '0009.german',\n",
      " 'dataset_path': './data',\n",
      " 'evaluate_option': 'best-val',\n",
      " 'gpu': '0',\n",
      " 'max_epoch': 200,\n",
      " 'model_path': './models',\n",
      " 'model_type': 'mlp',\n",
      " 'n_bins': 2,\n",
      " 'n_trials': 3,\n",
      " 'normalization': 'standard',\n",
      " 'num_nan_policy': 'mean',\n",
      " 'num_policy': 'none',\n",
      " 'retune': False,\n",
      " 'save_path': './models\\\\0009.german-mlp-Tune\\\\Epoch200BZ1024-Norm-standard-Nan-mean-new-Cat-ordinal',\n",
      " 'seed': 0,\n",
      " 'seed_num': 42,\n",
      " 'tune': True,\n",
      " 'use_float': False,\n",
      " 'workers': 0}\n",
      "\n",
      "Preprocessing configuration:\n",
      "  - cat_policy: ordinal\n",
      "  - num_policy: none\n",
      "  - normalization: standard\n",
      "  - num_nan_policy: mean\n",
      "  - cat_nan_policy: new\n",
      "\n",
      "Preparing data with 1 CV splits...\n",
      "\n",
      "======================================================================\n",
      "Fold 1/1\n",
      "======================================================================\n",
      "{'model': {'d_layers': [384, 384], 'dropout': 0.1}, 'training': {'lr': 0.0003, 'weight_decay': 1e-05, 'n_bins': 2}, 'general': {}, 'fit': {'max_epoch': 10}}\n",
      "epoch 0, train 1/1, loss=0.6895 lr=0.0003\n",
      "best epoch 0, best val res=0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, val, loss=0.6609 classification result=0.7063\n",
      "Epoch: 0, Time cost: 0.017267465591430664\n",
      "epoch 1, train 1/1, loss=0.6561 lr=0.0003\n",
      "best epoch 0, best val res=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 84.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, val, loss=0.6394 classification result=0.7000\n",
      "Epoch: 1, Time cost: 0.09967231750488281\n",
      "epoch 2, train 1/1, loss=0.6338 lr=0.0003\n",
      "best epoch 0, best val res=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 616.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, val, loss=0.6254 classification result=0.7000\n",
      "Epoch: 2, Time cost: 0.019840240478515625\n",
      "epoch 3, train 1/1, loss=0.6140 lr=0.0003\n",
      "best epoch 0, best val res=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, val, loss=0.6166 classification result=0.7000\n",
      "Epoch: 3, Time cost: 0.019533157348632812\n",
      "epoch 4, train 1/1, loss=0.6021 lr=0.0003\n",
      "best epoch 0, best val res=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 111.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, val, loss=0.6108 classification result=0.7000\n",
      "Epoch: 4, Time cost: 0.03428363800048828\n",
      "epoch 5, train 1/1, loss=0.5955 lr=0.0003\n",
      "best epoch 0, best val res=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 463.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, val, loss=0.6063 classification result=0.7000\n",
      "Epoch: 5, Time cost: 0.024171829223632812\n",
      "epoch 6, train 1/1, loss=0.5896 lr=0.0003\n",
      "best epoch 0, best val res=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 500.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, val, loss=0.6018 classification result=0.7000\n",
      "Epoch: 6, Time cost: 0.02463531494140625\n",
      "epoch 7, train 1/1, loss=0.5822 lr=0.0003\n",
      "best epoch 0, best val res=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 325.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, val, loss=0.5964 classification result=0.7000\n",
      "Epoch: 7, Time cost: 0.03395247459411621\n",
      "epoch 8, train 1/1, loss=0.5782 lr=0.0003\n",
      "best epoch 0, best val res=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 500.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, val, loss=0.5898 classification result=0.7000\n",
      "Epoch: 8, Time cost: 0.024580001831054688\n",
      "epoch 9, train 1/1, loss=0.5689 lr=0.0003\n",
      "best epoch 0, best val res=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 499.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, val, loss=0.5821 classification result=0.7000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9, Time cost: 0.024724960327148438\n",
      "best epoch 0, best val res=0.7063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 328.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: loss=0.6564\n",
      "[Accuracy]=0.7050\n",
      "[Avg_Recall]=0.5083\n",
      "[Avg_Precision]=0.8518\n",
      "[F1]=0.0328\n",
      "[LogLoss]=0.6564\n",
      "[AUC]=0.4946\n",
      "\n",
      "Fold 1 results:\n",
      "  Accuracy: 0.7050\n",
      "  Avg_Recall: 0.5083\n",
      "  Avg_Precision: 0.8518\n",
      "  F1: 0.0328\n",
      "  LogLoss: 0.6564\n",
      "  AUC: 0.4946\n",
      "\n",
      "======================================================================\n",
      "Completed 1 folds for mlp\n",
      "======================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RESULTS INSPECTION - FOLD 0\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRESULTS INSPECTION - FOLD 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m fold \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. RESULT DICT KEYS:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(fold\u001b[38;5;241m.\u001b[39mkeys()):\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE DEBUGGING CODE FOR method_runner\n",
    "# Run this in a Jupyter notebook cell in the notebooks/ folder\n",
    "# Copy-paste the entire code block below\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Setup path\n",
    "PROJECT_ROOT = Path.cwd().parents[0]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.methods.method_runner import run_talent_method, get_available_methods\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEBUGGING: TALENT method_runner Analysis\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 1: Single fold with HPO - inspect everything\n",
    "# ============================================================================\n",
    "print(\"\\nTEST 1: Single fold MLP + HPO (inspect results dict)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "results = run_talent_method(\n",
    "    task=\"pd\",\n",
    "    dataset=\"0009.german\",\n",
    "    test_size=0.2,\n",
    "    val_size=0.2,\n",
    "    cv_splits=1,\n",
    "    seed=42,\n",
    "    method=\"mlp\",\n",
    "    max_epoch=10,\n",
    "    tune=True,\n",
    "    n_trials=3,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESULTS INSPECTION - FOLD 0\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fold = results[0]\n",
    "\n",
    "print(\"\\n1. RESULT DICT KEYS:\")\n",
    "for key in sorted(fold.keys()):\n",
    "    print(f\"   - {key}: {type(fold[key])}\")\n",
    "\n",
    "print(\"\\n2. PREDICTIONS:\")\n",
    "print(f\"   y_true shape: {fold['y_true'].shape}\")\n",
    "print(f\"   y_true dtype: {fold['y_true'].dtype}\")\n",
    "print(f\"   y_true values: {np.unique(fold['y_true'])}\")\n",
    "print(f\"   y_true first 10: {fold['y_true'][:10]}\")\n",
    "\n",
    "print(f\"\\n   y_pred shape: {fold['y_pred'].shape}\")\n",
    "print(f\"   y_pred dtype: {fold['y_pred'].dtype}\")\n",
    "if fold['y_pred'].ndim == 1:\n",
    "    print(f\"   y_pred is 1D: {fold['y_pred'][:10]}\")\n",
    "else:\n",
    "    print(f\"   y_pred is {fold['y_pred'].ndim}D: {fold['y_pred'][:3]}\")\n",
    "\n",
    "print(\"\\n3. METRICS:\")\n",
    "print(f\"   Primary metric: {fold['primary_metric']}\")\n",
    "print(f\"   Metric names: {fold['metric_names']}\")\n",
    "print(f\"   Metrics: {fold['metrics']}\")\n",
    "if isinstance(fold['metrics'], (list, tuple)):\n",
    "    for name, val in zip(fold['metric_names'], fold['metrics']):\n",
    "        print(f\"      {name}: {val}\")\n",
    "\n",
    "print(\"\\n4. TRAINING INFO:\")\n",
    "print(f\"   Training time: {fold['train_time']:.2f}s\")\n",
    "print(f\"   Validation loss: {fold['val_loss']}\")\n",
    "print(f\"   Method: {fold['method']}\")\n",
    "print(f\"   Dataset: {fold['dataset']}\")\n",
    "\n",
    "print(\"\\n5. HPO CONFIG:\")\n",
    "hpo_config = fold['optimal_hpo_config']\n",
    "if hpo_config:\n",
    "    print(f\"   HPO config found: YES\")\n",
    "    print(f\"   Config structure: {json.dumps(hpo_config, indent=2)[:1000]}...\")\n",
    "else:\n",
    "    print(f\"   HPO config found: NO\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 2: 3 folds with HPO - check config reuse\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 2: 3 folds with HPO - check config reuse across folds\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_3fold = run_talent_method(\n",
    "    task=\"pd\",\n",
    "    dataset=\"0009.german\",\n",
    "    test_size=0.2,\n",
    "    val_size=0.2,\n",
    "    cv_splits=3,\n",
    "    seed=42,\n",
    "    method=\"mlp\",\n",
    "    max_epoch=10,\n",
    "    tune=True,\n",
    "    n_trials=3,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"\\nCROSS-FOLD ANALYSIS:\")\n",
    "configs = []\n",
    "metrics_list = []\n",
    "\n",
    "for fold_id, fold_result in results_3fold.items():\n",
    "    hpo_config = fold_result['optimal_hpo_config']\n",
    "    metric_names = fold_result['metric_names']\n",
    "    metrics = fold_result['metrics']\n",
    "    \n",
    "    configs.append(hpo_config)\n",
    "    metrics_list.append(dict(zip(metric_names, metrics)) if metric_names else {})\n",
    "    \n",
    "    print(f\"\\n   Fold {fold_id}:\")\n",
    "    print(f\"      Metrics: {metrics_list[-1]}\")\n",
    "    if hpo_config and 'mlp' in hpo_config:\n",
    "        if 'fit' in hpo_config['mlp']:\n",
    "            lr = hpo_config['mlp']['fit'].get('lr', 'N/A')\n",
    "            print(f\"      Tuned LR: {lr}\")\n",
    "\n",
    "print(\"\\n   CONFIG REUSE:\")\n",
    "config_0 = configs[0]\n",
    "for fold_id in range(1, len(configs)):\n",
    "    is_same = configs[fold_id] == config_0\n",
    "    print(f\"      Fold {fold_id} == Fold 0 config: {is_same}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 3: Compare WITH HPO vs WITHOUT HPO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TEST 3: WITH HPO vs WITHOUT HPO comparison\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n   Running WITHOUT HPO...\")\n",
    "results_no_hpo = run_talent_method(\n",
    "    task=\"pd\", dataset=\"0009.german\", test_size=0.2, val_size=0.2,\n",
    "    cv_splits=1, seed=42, method=\"mlp\", max_epoch=10,\n",
    "    tune=False, verbose=False,\n",
    ")\n",
    "\n",
    "print(\"   Running WITH HPO...\")\n",
    "results_with_hpo = run_talent_method(\n",
    "    task=\"pd\", dataset=\"0009.german\", test_size=0.2, val_size=0.2,\n",
    "    cv_splits=1, seed=42, method=\"mlp\", max_epoch=10,\n",
    "    tune=True, n_trials=3, verbose=False,\n",
    ")\n",
    "\n",
    "fold_no_hpo = results_no_hpo[0]\n",
    "fold_with_hpo = results_with_hpo[0]\n",
    "\n",
    "print(\"\\n   COMPARISON:\")\n",
    "print(f\"   Training time WITHOUT HPO: {fold_no_hpo['train_time']:.2f}s\")\n",
    "print(f\"   Training time WITH HPO:    {fold_with_hpo['train_time']:.2f}s\")\n",
    "\n",
    "metrics_no_hpo = dict(zip(fold_no_hpo['metric_names'], fold_no_hpo['metrics']))\n",
    "metrics_with_hpo = dict(zip(fold_with_hpo['metric_names'], fold_with_hpo['metrics']))\n",
    "\n",
    "print(f\"\\n   WITHOUT HPO metrics: {metrics_no_hpo}\")\n",
    "print(f\"   WITH HPO metrics:    {metrics_with_hpo}\")\n",
    "\n",
    "print(f\"\\n   Improvement:\")\n",
    "for metric_name in fold_no_hpo['metric_names']:\n",
    "    val_no_hpo = metrics_no_hpo[metric_name]\n",
    "    val_with_hpo = metrics_with_hpo[metric_name]\n",
    "    improvement = val_with_hpo - val_no_hpo\n",
    "    print(f\"      {metric_name}: {val_no_hpo:.4f} â†’ {val_with_hpo:.4f} ({improvement:+.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEBUG COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406383a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                TALENT METHOD RUNNER - COMPREHENSIVE TEST SUITE                 \n",
      "================================================================================\n",
      "\n",
      "Select test mode:\n",
      "  1. Full test suite (slow but comprehensive)\n",
      "  2. Quick HPO test (fast)\n",
      "  3. Quick comparison (HPO vs No HPO)\n",
      "  4. Debug single run (maximum verbosity)\n",
      "  5. Run all tests\n",
      "\n",
      "ðŸš€ Running FULL TEST SUITE...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "                  METHOD RUNNER TEST SUITE - WITH HPO SUPPORT                   \n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Test Configuration:\n",
      "  row_limit: 5000\n",
      "  cv_splits: 3\n",
      "  test_size: 0.2\n",
      "  val_size: 0.2\n",
      "  seed: 42\n",
      "  n_trials_hpo: 10\n",
      "\n",
      "ðŸ“‚ Datasets to test: 3\n",
      "   - 0001.gmsc\n",
      "   - 0002.aust\n",
      "   - 0003.japa\n",
      "\n",
      "ðŸ”§ Methods to test:\n",
      "   Total: 6 methods\n",
      "   - classical_with_hpo: xgboost, lightgbm, RandomForest\n",
      "   - deep_with_hpo: mlp, resnet\n",
      "   - zero_shot_no_hpo: tabpfn\n",
      "\n",
      "================================================================================\n",
      "                       TEST 1: CLASSICAL METHODS WITH HPO                       \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " Testing: xgboost on 0001.gmsc WITH HPO\n",
      "--------------------------------------------------------------------------------\n",
      "â³ Starting test...\n",
      "   Method: xgboost\n",
      "   Dataset: 0001.gmsc\n",
      "   Task: PD\n",
      "   HPO: Enabled\n",
      "   Row limit: 5000\n",
      "   CV splits: 3\n",
      "{'cat_min_frequency': 0.0,\n",
      " 'cat_nan_policy': 'new',\n",
      " 'cat_policy': 'ordinal',\n",
      " 'config': {'fit': {'verbose': False},\n",
      "            'model': {'booster': 'gbtree',\n",
      "                      'colsample_bytree': 0.8,\n",
      "                      'early_stopping_rounds': 50,\n",
      "                      'n_estimators': 2000,\n",
      "                      'n_jobs': -1,\n",
      "                      'subsample': 0.8,\n",
      "                      'tree_method': 'hist'}},\n",
      " 'dataset': '0001.gmsc',\n",
      " 'dataset_path': './data',\n",
      " 'evaluate_option': 'best-val',\n",
      " 'gpu': '0',\n",
      " 'model_path': './models',\n",
      " 'model_type': 'xgboost',\n",
      " 'n_bins': 2,\n",
      " 'n_trials': 10,\n",
      " 'normalization': 'standard',\n",
      " 'num_nan_policy': 'mean',\n",
      " 'num_policy': 'none',\n",
      " 'retune': False,\n",
      " 'save_path': './models\\\\0001.gmsc-xgboost-Tune\\\\Norm-standard-Nan-mean-new-Cat-ordinal',\n",
      " 'seed': 0,\n",
      " 'seed_num': 42,\n",
      " 'tune': True}\n",
      "\n",
      "âœ… SUCCESS!\n",
      "   Total time: 1.57s\n",
      "   Avg time per fold: 0.52s\n",
      "   Number of folds: 3\n",
      "\n",
      "ðŸ“Š Performance Summary:\n",
      "   Training time: 0.07s Â± 0.05s\n",
      "   Metric 0: 0.9372 Â± 0.0028\n",
      "   Metric 1: 0.5420 Â± 0.0154\n",
      "   Metric 2: 0.7053 Â± 0.0720\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " Testing: lightgbm on 0001.gmsc WITH HPO\n",
      "--------------------------------------------------------------------------------\n",
      "â³ Starting test...\n",
      "   Method: lightgbm\n",
      "   Dataset: 0001.gmsc\n",
      "   Task: PD\n",
      "   HPO: Enabled\n",
      "   Row limit: 5000\n",
      "   CV splits: 3\n",
      "{'cat_min_frequency': 0.0,\n",
      " 'cat_nan_policy': 'new',\n",
      " 'cat_policy': 'ordinal',\n",
      " 'config': {'fit': {}, 'model': {'n_estimators': 2000}},\n",
      " 'dataset': '0001.gmsc',\n",
      " 'dataset_path': './data',\n",
      " 'evaluate_option': 'best-val',\n",
      " 'gpu': '0',\n",
      " 'model_path': './models',\n",
      " 'model_type': 'lightgbm',\n",
      " 'n_bins': 2,\n",
      " 'n_trials': 10,\n",
      " 'normalization': 'standard',\n",
      " 'num_nan_policy': 'mean',\n",
      " 'num_policy': 'none',\n",
      " 'retune': False,\n",
      " 'save_path': './models\\\\0001.gmsc-lightgbm-Tune\\\\Norm-standard-Nan-mean-new-Cat-ordinal',\n",
      " 'seed': 0,\n",
      " 'seed_num': 42,\n",
      " 'tune': True}\n",
      "\n",
      "âœ… SUCCESS!\n",
      "   Total time: 5.61s\n",
      "   Avg time per fold: 1.87s\n",
      "   Number of folds: 3\n",
      "\n",
      "ðŸ“Š Performance Summary:\n",
      "   Training time: 1.46s Â± 0.66s\n",
      "   Metric 0: 0.9306 Â± 0.0042\n",
      "   Metric 1: 0.5615 Â± 0.0077\n",
      "   Metric 2: 0.6457 Â± 0.0300\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " Testing: RandomForest on 0001.gmsc WITH HPO\n",
      "--------------------------------------------------------------------------------\n",
      "â³ Starting test...\n",
      "   Method: RandomForest\n",
      "   Dataset: 0001.gmsc\n",
      "   Task: PD\n",
      "   HPO: Enabled\n",
      "   Row limit: 5000\n",
      "   CV splits: 3\n",
      "{'cat_min_frequency': 0.0,\n",
      " 'cat_nan_policy': 'new',\n",
      " 'cat_policy': 'ordinal',\n",
      " 'config': {'fit': {}, 'model': {'max_depth': 12, 'n_estimators': 2000}},\n",
      " 'dataset': '0001.gmsc',\n",
      " 'dataset_path': './data',\n",
      " 'evaluate_option': 'best-val',\n",
      " 'gpu': '0',\n",
      " 'model_path': './models',\n",
      " 'model_type': 'RandomForest',\n",
      " 'n_bins': 2,\n",
      " 'n_trials': 10,\n",
      " 'normalization': 'standard',\n",
      " 'num_nan_policy': 'mean',\n",
      " 'num_policy': 'none',\n",
      " 'retune': False,\n",
      " 'save_path': './models\\\\0001.gmsc-RandomForest-Tune\\\\Norm-standard-Nan-mean-new-Cat-ordinal',\n",
      " 'seed': 0,\n",
      " 'seed_num': 42,\n",
      " 'tune': True}\n",
      "\n",
      "âœ… SUCCESS!\n",
      "   Total time: 14.50s\n",
      "   Avg time per fold: 4.83s\n",
      "   Number of folds: 3\n",
      "\n",
      "ðŸ“Š Performance Summary:\n",
      "   Training time: 4.36s Â± 0.07s\n",
      "   Metric 0: 0.9360 Â± 0.0025\n",
      "   Metric 1: 0.5490 Â± 0.0132\n",
      "   Metric 2: 0.6803 Â± 0.0304\n",
      "\n",
      "================================================================================\n",
      "                TEST 2: CLASSICAL METHODS WITHOUT HPO (Baseline)                \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " Testing: xgboost on 0001.gmsc WITHOUT HPO\n",
      "--------------------------------------------------------------------------------\n",
      "â³ Starting test...\n",
      "   Method: xgboost\n",
      "   Dataset: 0001.gmsc\n",
      "   Task: PD\n",
      "   HPO: Disabled\n",
      "   Row limit: 5000\n",
      "   CV splits: 3\n",
      "{'cat_min_frequency': 0.0,\n",
      " 'cat_nan_policy': 'new',\n",
      " 'cat_policy': 'ordinal',\n",
      " 'config': {'fit': {'verbose': False},\n",
      "            'model': {'booster': 'gbtree',\n",
      "                      'colsample_bytree': 0.8,\n",
      "                      'early_stopping_rounds': 50,\n",
      "                      'n_estimators': 2000,\n",
      "                      'n_jobs': -1,\n",
      "                      'subsample': 0.8,\n",
      "                      'tree_method': 'hist'}},\n",
      " 'dataset': '0001.gmsc',\n",
      " 'dataset_path': './data',\n",
      " 'evaluate_option': 'best-val',\n",
      " 'gpu': '0',\n",
      " 'model_path': './models',\n",
      " 'model_type': 'xgboost',\n",
      " 'n_bins': 2,\n",
      " 'n_trials': 100,\n",
      " 'normalization': 'standard',\n",
      " 'num_nan_policy': 'mean',\n",
      " 'num_policy': 'none',\n",
      " 'retune': False,\n",
      " 'save_path': './models\\\\0001.gmsc-xgboost\\\\Norm-standard-Nan-mean-new-Cat-ordinal',\n",
      " 'seed': 0,\n",
      " 'seed_num': 42,\n",
      " 'tune': False}\n",
      "\n",
      "âœ… SUCCESS!\n",
      "   Total time: 0.18s\n",
      "   Avg time per fold: 0.06s\n",
      "   Number of folds: 3\n",
      "\n",
      "ðŸ“Š Performance Summary:\n",
      "   Training time: 0.04s Â± 0.00s\n",
      "   Metric 0: 0.9372 Â± 0.0028\n",
      "   Metric 1: 0.5420 Â± 0.0154\n",
      "   Metric 2: 0.7053 Â± 0.0720\n",
      "\n",
      "================================================================================\n",
      "                     TEST 3: DEEP LEARNING METHODS WITH HPO                     \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " Testing: mlp on 0001.gmsc WITH HPO\n",
      "--------------------------------------------------------------------------------\n",
      "â³ Starting test...\n",
      "   Method: mlp\n",
      "   Dataset: 0001.gmsc\n",
      "   Task: PD\n",
      "   HPO: Enabled\n",
      "   Row limit: 5000\n",
      "   CV splits: 3\n",
      "{'batch_size': 1024,\n",
      " 'cat_min_frequency': 0.0,\n",
      " 'cat_nan_policy': 'new',\n",
      " 'cat_policy': 'ordinal',\n",
      " 'config': {'general': {},\n",
      "            'model': {'d_layers': [384, 384], 'dropout': 0.1},\n",
      "            'training': {'lr': 0.0003, 'weight_decay': 1e-05}},\n",
      " 'dataset': '0001.gmsc',\n",
      " 'dataset_path': './data',\n",
      " 'evaluate_option': 'best-val',\n",
      " 'gpu': '0',\n",
      " 'max_epoch': 200,\n",
      " 'model_path': './models',\n",
      " 'model_type': 'mlp',\n",
      " 'n_bins': 2,\n",
      " 'n_trials': 10,\n",
      " 'normalization': 'standard',\n",
      " 'num_nan_policy': 'mean',\n",
      " 'num_policy': 'none',\n",
      " 'retune': False,\n",
      " 'save_path': './models\\\\0001.gmsc-mlp-Tune\\\\Epoch200BZ1024-Norm-standard-Nan-mean-new-Cat-ordinal',\n",
      " 'seed': 0,\n",
      " 'seed_num': 42,\n",
      " 'tune': True,\n",
      " 'use_float': False,\n",
      " 'workers': 0}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comprehensive Test Suite for method_runner.py with HPO\n",
    "======================================================\n",
    "\n",
    "This notebook tests the method_runner with:\n",
    "- Multiple datasets\n",
    "- Multiple methods (with and without HPO support)\n",
    "- HPO enabled and disabled\n",
    "- Extensive debugging output\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Ensure we can import from the src folder (project root = parent of notebooks/)\n",
    "PROJECT_ROOT = Path.cwd().parents[0]\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "from src.methods.method_runner import run_talent_method, get_available_methods\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Test configuration\n",
    "TEST_CONFIG = {\n",
    "    \"row_limit\": 5000,           # Limit rows for faster testing\n",
    "    \"cv_splits\": 3,              # Number of CV folds\n",
    "    \"test_size\": 0.2,            # Test set size\n",
    "    \"val_size\": 0.2,             # Validation set size\n",
    "    \"seed\": 42,                  # Random seed\n",
    "    \"n_trials_hpo\": 10,          # HPO trials (reduced for testing)\n",
    "}\n",
    "\n",
    "# Datasets to test\n",
    "DATASETS = [\n",
    "    \"0001.gmsc\",     # German Credit\n",
    "    \"0002.aust\",     # Australian Credit\n",
    "    \"0003.japa\",     # Japanese Credit\n",
    "]\n",
    "\n",
    "# Methods to test\n",
    "METHODS_TO_TEST = {\n",
    "    # Classical methods (support HPO)\n",
    "    \"classical_with_hpo\": [\n",
    "        \"xgboost\",\n",
    "        \"lightgbm\",\n",
    "        \"RandomForest\",\n",
    "    ],\n",
    "    \n",
    "    # Deep methods (support HPO)  \n",
    "    \"deep_with_hpo\": [\n",
    "        \"mlp\",\n",
    "        \"resnet\",\n",
    "    ],\n",
    "    \n",
    "    # Zero-shot methods (NO HPO support)\n",
    "    \"zero_shot_no_hpo\": [\n",
    "        \"tabpfn\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def print_section(title, char=\"=\", width=80):\n",
    "    \"\"\"Print a formatted section header.\"\"\"\n",
    "    print(f\"\\n{char * width}\")\n",
    "    print(f\"{title.center(width)}\")\n",
    "    print(f\"{char * width}\\n\")\n",
    "\n",
    "\n",
    "def print_subsection(title, char=\"-\", width=80):\n",
    "    \"\"\"Print a formatted subsection header.\"\"\"\n",
    "    print(f\"\\n{char * width}\")\n",
    "    print(f\" {title}\")\n",
    "    print(f\"{char * width}\")\n",
    "\n",
    "\n",
    "def print_dict(d, indent=0):\n",
    "    \"\"\"Pretty print a dictionary.\"\"\"\n",
    "    for key, value in d.items():\n",
    "        print(\"  \" * indent + f\"{key}:\", end=\" \")\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            print_dict(value, indent + 1)\n",
    "        elif isinstance(value, (list, tuple)) and len(str(value)) > 60:\n",
    "            print(f\"{type(value).__name__} with {len(value)} items\")\n",
    "        else:\n",
    "            print(value)\n",
    "\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format time in seconds to human-readable string.\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.2f}s\"\n",
    "    elif seconds < 3600:\n",
    "        return f\"{seconds/60:.2f}m\"\n",
    "    else:\n",
    "        return f\"{seconds/3600:.2f}h\"\n",
    "\n",
    "\n",
    "def compute_metrics_summary(results):\n",
    "    \"\"\"Compute summary statistics across folds.\"\"\"\n",
    "    train_times = [r['train_time'] for r in results.values()]\n",
    "    \n",
    "    # Extract metrics if they exist\n",
    "    metrics_dict = {}\n",
    "    for fold_id, fold_results in results.items():\n",
    "        if isinstance(fold_results.get('metrics'), (tuple, list)):\n",
    "            for i, metric in enumerate(fold_results['metrics']):\n",
    "                if f'metric_{i}' not in metrics_dict:\n",
    "                    metrics_dict[f'metric_{i}'] = []\n",
    "                metrics_dict[f'metric_{i}'].append(metric)\n",
    "    \n",
    "    summary = {\n",
    "        'n_folds': len(results),\n",
    "        'train_time_mean': np.mean(train_times),\n",
    "        'train_time_std': np.std(train_times),\n",
    "        'train_time_total': np.sum(train_times),\n",
    "    }\n",
    "    \n",
    "    # Add metric summaries\n",
    "    for metric_name, values in metrics_dict.items():\n",
    "        summary[f'{metric_name}_mean'] = np.mean(values)\n",
    "        summary[f'{metric_name}_std'] = np.std(values)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TEST FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def test_method_on_dataset(\n",
    "    method: str,\n",
    "    dataset: str,\n",
    "    task: str = \"pd\",\n",
    "    tune: bool = False,\n",
    "    verbose: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Test a single method on a single dataset.\n",
    "    \n",
    "    Args:\n",
    "        method: Method name (TALENT canonical)\n",
    "        dataset: Dataset name\n",
    "        task: Task type ('pd' or 'lgd')\n",
    "        tune: Whether to enable HPO\n",
    "        verbose: Whether to print verbose output\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with test results and timing info\n",
    "    \"\"\"\n",
    "    print_subsection(f\"Testing: {method} on {dataset} {'WITH' if tune else 'WITHOUT'} HPO\")\n",
    "    \n",
    "    test_result = {\n",
    "        'method': method,\n",
    "        'dataset': dataset,\n",
    "        'task': task,\n",
    "        'tune': tune,\n",
    "        'success': False,\n",
    "        'error': None,\n",
    "        'results': None,\n",
    "        'timing': {},\n",
    "        'summary': {},\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"â³ Starting test...\")\n",
    "        print(f\"   Method: {method}\")\n",
    "        print(f\"   Dataset: {dataset}\")\n",
    "        print(f\"   Task: {task.upper()}\")\n",
    "        print(f\"   HPO: {'Enabled' if tune else 'Disabled'}\")\n",
    "        print(f\"   Row limit: {TEST_CONFIG['row_limit']}\")\n",
    "        print(f\"   CV splits: {TEST_CONFIG['cv_splits']}\")\n",
    "        \n",
    "        # Run the method\n",
    "        results = run_talent_method(\n",
    "            task=task,\n",
    "            dataset=dataset,\n",
    "            test_size=TEST_CONFIG['test_size'],\n",
    "            val_size=TEST_CONFIG['val_size'],\n",
    "            cv_splits=TEST_CONFIG['cv_splits'],\n",
    "            seed=TEST_CONFIG['seed'],\n",
    "            row_limit=TEST_CONFIG['row_limit'],\n",
    "            method=method,\n",
    "            tune=tune,\n",
    "            n_trials=TEST_CONFIG['n_trials_hpo'] if tune else 0,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        \n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Compute summary\n",
    "        summary = compute_metrics_summary(results)\n",
    "        \n",
    "        # Store results\n",
    "        test_result['success'] = True\n",
    "        test_result['results'] = results\n",
    "        test_result['timing'] = {\n",
    "            'total_time': total_time,\n",
    "            'per_fold_avg': total_time / len(results),\n",
    "        }\n",
    "        test_result['summary'] = summary\n",
    "        \n",
    "        # Print success info\n",
    "        print(f\"\\nâœ… SUCCESS!\")\n",
    "        print(f\"   Total time: {format_time(total_time)}\")\n",
    "        print(f\"   Avg time per fold: {format_time(total_time / len(results))}\")\n",
    "        print(f\"   Number of folds: {len(results)}\")\n",
    "        \n",
    "        if summary:\n",
    "            print(f\"\\nðŸ“Š Performance Summary:\")\n",
    "            print(f\"   Training time: {summary['train_time_mean']:.2f}s Â± {summary['train_time_std']:.2f}s\")\n",
    "            \n",
    "            # Print first few metrics\n",
    "            metric_keys = [k for k in summary.keys() if k.startswith('metric_') and k.endswith('_mean')]\n",
    "            for i, metric_key in enumerate(metric_keys[:3]):  # First 3 metrics\n",
    "                mean_val = summary[metric_key]\n",
    "                std_key = metric_key.replace('_mean', '_std')\n",
    "                std_val = summary[std_key]\n",
    "                print(f\"   Metric {i}: {mean_val:.4f} Â± {std_val:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Record error\n",
    "        test_result['success'] = False\n",
    "        test_result['error'] = str(e)\n",
    "        \n",
    "        print(f\"\\nâŒ FAILED!\")\n",
    "        print(f\"   Error: {e}\")\n",
    "        print(f\"   Error type: {type(e).__name__}\")\n",
    "        \n",
    "        # Print traceback for debugging\n",
    "        import traceback\n",
    "        print(f\"\\nðŸ› Traceback:\")\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return test_result\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TEST SUITE\n",
    "# =============================================================================\n",
    "\n",
    "def run_test_suite():\n",
    "    \"\"\"Run the complete test suite.\"\"\"\n",
    "    \n",
    "    print_section(\"METHOD RUNNER TEST SUITE - WITH HPO SUPPORT\", \"=\", 80)\n",
    "    \n",
    "    # Print configuration\n",
    "    print(\"ðŸ“‹ Test Configuration:\")\n",
    "    print_dict(TEST_CONFIG, indent=1)\n",
    "    \n",
    "    print(f\"\\nðŸ“‚ Datasets to test: {len(DATASETS)}\")\n",
    "    for dataset in DATASETS:\n",
    "        print(f\"   - {dataset}\")\n",
    "    \n",
    "    print(f\"\\nðŸ”§ Methods to test:\")\n",
    "    total_methods = sum(len(methods) for methods in METHODS_TO_TEST.values())\n",
    "    print(f\"   Total: {total_methods} methods\")\n",
    "    for category, methods in METHODS_TO_TEST.items():\n",
    "        print(f\"   - {category}: {', '.join(methods)}\")\n",
    "    \n",
    "    # Storage for all results\n",
    "    all_results = []\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # TEST 1: Classical Methods WITH HPO\n",
    "    # ==========================================================================\n",
    "    \n",
    "    print_section(\"TEST 1: CLASSICAL METHODS WITH HPO\", \"=\")\n",
    "    \n",
    "    for method in METHODS_TO_TEST['classical_with_hpo']:\n",
    "        for dataset in DATASETS[:1]:  # Test on first dataset only for speed\n",
    "            result = test_method_on_dataset(\n",
    "                method=method,\n",
    "                dataset=dataset,\n",
    "                task=\"pd\",\n",
    "                tune=True,  # Enable HPO\n",
    "                verbose=False,\n",
    "            )\n",
    "            all_results.append(result)\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # TEST 2: Classical Methods WITHOUT HPO (for comparison)\n",
    "    # ==========================================================================\n",
    "    \n",
    "    print_section(\"TEST 2: CLASSICAL METHODS WITHOUT HPO (Baseline)\", \"=\")\n",
    "    \n",
    "    for method in METHODS_TO_TEST['classical_with_hpo'][:1]:  # Just test one\n",
    "        for dataset in DATASETS[:1]:\n",
    "            result = test_method_on_dataset(\n",
    "                method=method,\n",
    "                dataset=dataset,\n",
    "                task=\"pd\",\n",
    "                tune=False,  # Disable HPO\n",
    "                verbose=False,\n",
    "            )\n",
    "            all_results.append(result)\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # TEST 3: Deep Methods WITH HPO\n",
    "    # ==========================================================================\n",
    "    \n",
    "    print_section(\"TEST 3: DEEP LEARNING METHODS WITH HPO\", \"=\")\n",
    "    \n",
    "    for method in METHODS_TO_TEST['deep_with_hpo']:\n",
    "        for dataset in DATASETS[:1]:  # Test on first dataset only\n",
    "            result = test_method_on_dataset(\n",
    "                method=method,\n",
    "                dataset=dataset,\n",
    "                task=\"pd\",\n",
    "                tune=True,  # Enable HPO\n",
    "                verbose=False,\n",
    "            )\n",
    "            all_results.append(result)\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # TEST 4: Zero-Shot Methods (Should NOT use HPO)\n",
    "    # ==========================================================================\n",
    "    \n",
    "    print_section(\"TEST 4: ZERO-SHOT METHODS (NO HPO)\", \"=\")\n",
    "    \n",
    "    for method in METHODS_TO_TEST['zero_shot_no_hpo']:\n",
    "        for dataset in DATASETS[:1]:\n",
    "            result = test_method_on_dataset(\n",
    "                method=method,\n",
    "                dataset=dataset,\n",
    "                task=\"pd\",\n",
    "                tune=False,  # MUST be False for TabPFN\n",
    "                verbose=False,\n",
    "            )\n",
    "            all_results.append(result)\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # TEST 5: Error Case - Try HPO with TabPFN (Should Fail)\n",
    "    # ==========================================================================\n",
    "    \n",
    "    print_section(\"TEST 5: ERROR HANDLING - HPO with TabPFN (Expected to Fail)\", \"=\")\n",
    "    \n",
    "    print(\"âš ï¸  This test is EXPECTED to fail (TabPFN doesn't support HPO)\")\n",
    "    print(\"   We're testing that the error handling works correctly.\\n\")\n",
    "    \n",
    "    result = test_method_on_dataset(\n",
    "        method=\"tabpfn\",\n",
    "        dataset=DATASETS[0],\n",
    "        task=\"pd\",\n",
    "        tune=True,  # This should cause an error\n",
    "        verbose=False,\n",
    "    )\n",
    "    all_results.append(result)\n",
    "    \n",
    "    if not result['success']:\n",
    "        print(\"\\nâœ… Error handling worked correctly!\")\n",
    "        print(f\"   Expected error occurred: {result['error']}\")\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # FINAL SUMMARY\n",
    "    # ==========================================================================\n",
    "    \n",
    "    print_section(\"FINAL SUMMARY\", \"=\")\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_data = []\n",
    "    for result in all_results:\n",
    "        row = {\n",
    "            'method': result['method'],\n",
    "            'dataset': result['dataset'],\n",
    "            'hpo': 'Yes' if result['tune'] else 'No',\n",
    "            'success': 'âœ“' if result['success'] else 'âœ—',\n",
    "            'total_time': result['timing'].get('total_time', np.nan),\n",
    "            'error': result['error'] if result['error'] else '',\n",
    "        }\n",
    "        \n",
    "        # Add performance metrics if available\n",
    "        if result['summary']:\n",
    "            row['train_time_mean'] = result['summary'].get('train_time_mean', np.nan)\n",
    "            row['metric_0_mean'] = result['summary'].get('metric_0_mean', np.nan)\n",
    "            row['metric_5_mean'] = result['summary'].get('metric_5_mean', np.nan)  # Usually AUC\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Test Results Summary:\")\n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    # Success rate\n",
    "    n_success = sum(1 for r in all_results if r['success'])\n",
    "    n_total = len(all_results)\n",
    "    success_rate = n_success / n_total * 100\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Overall Success Rate: {n_success}/{n_total} ({success_rate:.1f}%)\")\n",
    "    \n",
    "    # HPO comparison\n",
    "    print(\"\\nðŸ” HPO Impact Analysis:\")\n",
    "    \n",
    "    hpo_results = [r for r in all_results if r['tune'] and r['success']]\n",
    "    no_hpo_results = [r for r in all_results if not r['tune'] and r['success']]\n",
    "    \n",
    "    if hpo_results and no_hpo_results:\n",
    "        hpo_times = [r['timing']['total_time'] for r in hpo_results]\n",
    "        no_hpo_times = [r['timing']['total_time'] for r in no_hpo_results]\n",
    "        \n",
    "        print(f\"   WITH HPO:\")\n",
    "        print(f\"      Avg time: {np.mean(hpo_times):.2f}s\")\n",
    "        print(f\"      Tests: {len(hpo_results)}\")\n",
    "        \n",
    "        print(f\"   WITHOUT HPO:\")\n",
    "        print(f\"      Avg time: {np.mean(no_hpo_times):.2f}s\")\n",
    "        print(f\"      Tests: {len(no_hpo_results)}\")\n",
    "        \n",
    "        speedup = np.mean(hpo_times) / np.mean(no_hpo_times)\n",
    "        print(f\"   Time ratio: {speedup:.1f}x slower with HPO\")\n",
    "    \n",
    "    # Method comparison\n",
    "    print(\"\\nðŸ† Method Performance Ranking (by metric_0):\")\n",
    "    \n",
    "    successful_results = [r for r in all_results if r['success'] and r['summary']]\n",
    "    if successful_results:\n",
    "        method_perf = []\n",
    "        for result in successful_results:\n",
    "            if 'metric_0_mean' in result['summary']:\n",
    "                method_perf.append({\n",
    "                    'method': result['method'],\n",
    "                    'hpo': 'Yes' if result['tune'] else 'No',\n",
    "                    'metric_0': result['summary']['metric_0_mean'],\n",
    "                })\n",
    "        \n",
    "        if method_perf:\n",
    "            df_perf = pd.DataFrame(method_perf).sort_values('metric_0', ascending=False)\n",
    "            print(df_perf.to_string(index=False))\n",
    "    \n",
    "    print_section(\"TEST SUITE COMPLETE!\", \"=\")\n",
    "    \n",
    "    return all_results, df_summary\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEBUGGING HELPER\n",
    "# =============================================================================\n",
    "\n",
    "def debug_single_run(method=\"mlp\", dataset=\"0001.gmsc\", tune=True):\n",
    "    \"\"\"\n",
    "    Run a single method with maximum verbosity for debugging.\n",
    "    \n",
    "    Args:\n",
    "        method: Method to test\n",
    "        dataset: Dataset to use\n",
    "        tune: Enable HPO\n",
    "    \"\"\"\n",
    "    print_section(f\"DEBUG RUN: {method} on {dataset}\", \"=\")\n",
    "    \n",
    "    print(\"ðŸ” Running with MAXIMUM verbosity for debugging...\\n\")\n",
    "    \n",
    "    # Import method_runner internals for inspection\n",
    "    from src.methods.method_runner import (\n",
    "        get_available_methods,\n",
    "        validate_method,\n",
    "        DEEP_METHODS,\n",
    "        CLASSICAL_METHODS\n",
    "    )\n",
    "    \n",
    "    # Validate method\n",
    "    print(\"1ï¸âƒ£ VALIDATING METHOD\")\n",
    "    try:\n",
    "        validated_method, is_deep = validate_method(method)\n",
    "        print(f\"   âœ“ Method '{method}' is valid\")\n",
    "        print(f\"   âœ“ Type: {'Deep Learning' if is_deep else 'Classical'}\")\n",
    "        print(f\"   âœ“ Supports HPO: {'Yes' if method not in ['tabpfn', 'PFN-v2', 'tabptm'] else 'No'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âœ— Validation failed: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Check dataset\n",
    "    print(f\"\\n2ï¸âƒ£ CHECKING DATASET\")\n",
    "    print(f\"   Dataset: {dataset}\")\n",
    "    print(f\"   Row limit: {TEST_CONFIG['row_limit']}\")\n",
    "    \n",
    "    # Run with verbose output\n",
    "    print(f\"\\n3ï¸âƒ£ RUNNING METHOD\")\n",
    "    print(f\"   HPO: {'ENABLED' if tune else 'DISABLED'}\")\n",
    "    if tune:\n",
    "        print(f\"   Number of trials: {TEST_CONFIG['n_trials_hpo']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        results = run_talent_method(\n",
    "            task=\"pd\",\n",
    "            dataset=dataset,\n",
    "            test_size=0.2,\n",
    "            val_size=0.2,\n",
    "            cv_splits=3,\n",
    "            seed=42,\n",
    "            row_limit=TEST_CONFIG['row_limit'],\n",
    "            method=method,\n",
    "            tune=tune,\n",
    "            n_trials=TEST_CONFIG['n_trials_hpo'],\n",
    "            verbose=True,  # MAXIMUM VERBOSITY\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n4ï¸âƒ£ RESULTS\")\n",
    "        print(f\"   âœ“ Completed successfully\")\n",
    "        print(f\"   âœ“ Time: {format_time(elapsed)}\")\n",
    "        print(f\"   âœ“ Number of folds: {len(results)}\")\n",
    "        \n",
    "        # Inspect first fold in detail\n",
    "        print(f\"\\n5ï¸âƒ£ DETAILED INSPECTION OF FOLD 1\")\n",
    "        fold_1 = results[1]\n",
    "        \n",
    "        print(f\"   Keys: {list(fold_1.keys())}\")\n",
    "        print(f\"   Method: {fold_1['method']}\")\n",
    "        print(f\"   Dataset: {fold_1['dataset']}\")\n",
    "        print(f\"   Task: {fold_1['task']}\")\n",
    "        print(f\"   Training time: {fold_1['train_time']:.2f}s\")\n",
    "        print(f\"   y_true shape: {fold_1['y_true'].shape}\")\n",
    "        print(f\"   y_pred shape: {fold_1['y_pred'].shape}\")\n",
    "        print(f\"   Metrics: {fold_1['metrics']}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n   âœ— FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# QUICK TEST FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def quick_test_hpo():\n",
    "    \"\"\"Quick test to verify HPO is working.\"\"\"\n",
    "    print_section(\"QUICK HPO TEST\", \"=\")\n",
    "    \n",
    "    print(\"Testing XGBoost with HPO on small dataset...\\n\")\n",
    "    \n",
    "    result = test_method_on_dataset(\n",
    "        method=\"xgboost\",\n",
    "        dataset=\"0001.gmsc\",\n",
    "        task=\"pd\",\n",
    "        tune=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(\"\\nâœ… HPO test PASSED!\")\n",
    "        print(f\"   Check if HPO improved performance by comparing with baseline\")\n",
    "    else:\n",
    "        print(\"\\nâŒ HPO test FAILED!\")\n",
    "        print(f\"   Error: {result['error']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def quick_test_comparison():\n",
    "    \"\"\"Quick test comparing with and without HPO.\"\"\"\n",
    "    print_section(\"QUICK COMPARISON: HPO vs No HPO\", \"=\")\n",
    "    \n",
    "    method = \"xgboost\"\n",
    "    dataset = \"0001.gmsc\"\n",
    "    \n",
    "    print(f\"Testing {method} on {dataset}...\\n\")\n",
    "    \n",
    "    # Without HPO\n",
    "    print(\"1ï¸âƒ£ WITHOUT HPO (baseline):\")\n",
    "    result_no_hpo = test_method_on_dataset(\n",
    "        method=method,\n",
    "        dataset=dataset,\n",
    "        task=\"pd\",\n",
    "        tune=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    # With HPO\n",
    "    print(\"\\n2ï¸âƒ£ WITH HPO:\")\n",
    "    result_with_hpo = test_method_on_dataset(\n",
    "        method=method,\n",
    "        dataset=dataset,\n",
    "        task=\"pd\",\n",
    "        tune=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    # Compare\n",
    "    print_section(\"COMPARISON RESULTS\", \"-\")\n",
    "    \n",
    "    if result_no_hpo['success'] and result_with_hpo['success']:\n",
    "        no_hpo_metric = result_no_hpo['summary'].get('metric_0_mean', 0)\n",
    "        with_hpo_metric = result_with_hpo['summary'].get('metric_0_mean', 0)\n",
    "        \n",
    "        improvement = (with_hpo_metric - no_hpo_metric) / no_hpo_metric * 100\n",
    "        \n",
    "        print(f\"Without HPO: metric = {no_hpo_metric:.4f}\")\n",
    "        print(f\"With HPO:    metric = {with_hpo_metric:.4f}\")\n",
    "        print(f\"Improvement: {improvement:+.2f}%\")\n",
    "        \n",
    "        no_hpo_time = result_no_hpo['timing']['total_time']\n",
    "        with_hpo_time = result_with_hpo['timing']['total_time']\n",
    "        time_ratio = with_hpo_time / no_hpo_time\n",
    "        \n",
    "        print(f\"\\nWithout HPO: time = {format_time(no_hpo_time)}\")\n",
    "        print(f\"With HPO:    time = {format_time(with_hpo_time)}\")\n",
    "        print(f\"Time ratio:  {time_ratio:.1f}x slower\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Could not compare due to errors\")\n",
    "    \n",
    "    return result_no_hpo, result_with_hpo\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" TALENT METHOD RUNNER - COMPREHENSIVE TEST SUITE \".center(80))\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    print(\"Select test mode:\")\n",
    "    print(\"  1. Full test suite (slow but comprehensive)\")\n",
    "    print(\"  2. Quick HPO test (fast)\")\n",
    "    print(\"  3. Quick comparison (HPO vs No HPO)\")\n",
    "    print(\"  4. Debug single run (maximum verbosity)\")\n",
    "    print(\"  5. Run all tests\")\n",
    "    \n",
    "    # For notebook execution, uncomment one of these:\n",
    "    \n",
    "    # Option 1: Full test suite\n",
    "    # all_results, df_summary = run_test_suite()\n",
    "    \n",
    "    # Option 2: Quick HPO test\n",
    "    # result = quick_test_hpo()\n",
    "    \n",
    "    # Option 3: Quick comparison\n",
    "    # result_no_hpo, result_with_hpo = quick_test_comparison()\n",
    "    \n",
    "    # Option 4: Debug run\n",
    "    # results = debug_single_run(method=\"mlp\", dataset=\"0001.gmsc\", tune=True)\n",
    "    \n",
    "    # Option 5: Run all\n",
    "    print(\"\\nðŸš€ Running FULL TEST SUITE...\\n\")\n",
    "    all_results, df_summary = run_test_suite()\n",
    "    \n",
    "    print(\"\\nâœ… All tests complete! Results stored in 'all_results' and 'df_summary'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a87dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_talent_method(\n",
    "    task=\"pd\", dataset=\"adult\", test_size=0.2, val_size=0.2,\n",
    "    cv_splits=1, seed=42, method=\"mlp\", max_epoch=5, verbose=True\n",
    ")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
