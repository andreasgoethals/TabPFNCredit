{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbcaafe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " COMPREHENSIVE TALENT METHOD TESTING\n",
      "================================================================================\n",
      "\n",
      "Start time: 2025-11-13 14:14:33\n",
      "\n",
      "üìã Configuration:\n",
      "  Datasets:\n",
      "    PD:  0001.gmsc\n",
      "    LGD: 0001.heloc\n",
      "  Data:\n",
      "    Row limit: 1000\n",
      "    CV splits: 3\n",
      "    Test/Val size: 0.2/0.2\n",
      "  Training:\n",
      "    Max epochs: 50\n",
      "    Batch size: 256\n",
      "  HPO:\n",
      "    Trials: 100\n",
      "    Run with HPO: True\n",
      "    Run without HPO: True\n",
      "\n",
      "üìä Methods to test:\n",
      "  PD: 15 methods\n",
      "    Classical: catboost, knn, lightgbm, LogReg, NaiveBayes, RandomForest, svm, xgboost, NCM, dummy\n",
      "    Deep: mlp, tabnet, tabpfn, snn, dcn2\n",
      "  LGD: 9 methods\n",
      "    Classical: catboost, knn, lightgbm, LinearRegression, RandomForest, xgboost\n",
      "    Deep: mlp, tabnet, PFN-v2\n",
      "\n",
      "================================================================================\n",
      " TESTING PD DATASET: 0001.gmsc\n",
      "================================================================================\n",
      "Testing 15 methods...\n",
      "  üîÑ PD_catboost_DEFAULT... ‚úì AUC=0.8119 (5.5s)\n",
      "  üîÑ PD_catboost_HPO... ‚úì AUC=0.8037 (2.3s)\n",
      "  üîÑ PD_knn_DEFAULT... ‚úì AUC=0.6176 (0.1s)\n",
      "  üîÑ PD_knn_HPO... ‚úì AUC=0.7549 (0.1s)\n",
      "  üîÑ PD_lightgbm_DEFAULT... ‚úì AUC=0.7402 (3.6s)\n",
      "  üîÑ PD_lightgbm_HPO... ‚úì AUC=0.7526 (0.2s)\n",
      "  üîÑ PD_LogReg_DEFAULT... ‚úì AUC=0.7311 (0.1s)\n",
      "  üîÑ PD_LogReg_HPO... ‚úì AUC=0.7983 (0.1s)\n",
      "  üîÑ PD_NaiveBayes_DEFAULT... ‚úì AUC=0.6495 (0.1s)\n",
      "  ‚äò PD_NaiveBayes_HPO... Skipped (no HPO support)\n",
      "  üîÑ PD_RandomForest_DEFAULT... ‚úì AUC=0.8065 (5.2s)\n",
      "  üîÑ PD_RandomForest_HPO... ‚úì AUC=0.8213 (0.3s)\n",
      "  üîÑ PD_svm_DEFAULT... ‚úì AUC=0.5061 (0.1s)\n",
      "  üîÑ PD_svm_HPO... ‚úì AUC=0.5061 (0.1s)\n",
      "  üîÑ PD_xgboost_DEFAULT... ‚úì AUC=0.7315 (1.0s)\n",
      "  üîÑ PD_xgboost_HPO... ‚úì AUC=0.7759 (0.2s)\n",
      "  üîÑ PD_NCM_DEFAULT... ‚úì AUC=0.6849 (0.2s)\n",
      "  ‚äò PD_NCM_HPO... Skipped (no HPO support)\n",
      "  üîÑ PD_dummy_DEFAULT... ‚úì AUC=0.5000 (0.1s)\n",
      "  ‚äò PD_dummy_HPO... Skipped (no HPO support)\n",
      "  üîÑ PD_mlp_DEFAULT... ‚úì AUC=0.7675 (4.1s)\n",
      "  üîÑ PD_mlp_HPO... ‚úì AUC=0.6930 (4.1s)\n",
      "  üîÑ PD_tabnet_DEFAULT... ‚úì AUC=0.4695 (5.2s)\n",
      "  üîÑ PD_tabnet_HPO... ‚úì AUC=0.4939 (16.4s)\n",
      "  üîÑ PD_tabpfn_DEFAULT... ‚úì AUC=0.8081 (0.6s)\n",
      "  ‚äò PD_tabpfn_HPO... Skipped (no HPO support)\n",
      "  üîÑ PD_snn_DEFAULT... ‚úì AUC=0.6986 (3.0s)\n",
      "  üîÑ PD_snn_HPO... ‚úì AUC=0.5829 (4.7s)\n",
      "  üîÑ PD_dcn2_DEFAULT... ‚úì AUC=0.5595 (4.8s)\n",
      "  üîÑ PD_dcn2_HPO... ‚úì AUC=0.5973 (2.4s)\n",
      "\n",
      "================================================================================\n",
      " TESTING LGD DATASET: 0001.heloc\n",
      "================================================================================\n",
      "Testing 9 methods...\n",
      "  üîÑ LGD_catboost_DEFAULT... ‚úì MSE=0.3586 (3.8s)\n",
      "  üîÑ LGD_catboost_HPO... ‚úì MSE=0.3663 (2.7s)\n",
      "  üîÑ LGD_knn_DEFAULT... ‚úì MSE=0.4124 (0.1s)\n",
      "  üîÑ LGD_knn_HPO... ‚úì MSE=0.3039 (0.1s)\n",
      "  üîÑ LGD_lightgbm_DEFAULT... ‚úì MSE=0.3898 (3.7s)\n",
      "  üîÑ LGD_lightgbm_HPO... ‚úì MSE=0.3875 (0.5s)\n",
      "  üîÑ LGD_LinearRegression_DEFAULT... ‚úì MSE=0.3334 (0.0s)\n",
      "  ‚äò LGD_LinearRegression_HPO... Skipped (no HPO support)\n",
      "  üîÑ LGD_RandomForest_DEFAULT... ‚úì MSE=0.3357 (5.8s)\n",
      "  üîÑ LGD_RandomForest_HPO... ‚úì MSE=0.3374 (0.3s)\n",
      "  üîÑ LGD_xgboost_DEFAULT... ‚úì MSE=0.3741 (0.3s)\n",
      "  üîÑ LGD_xgboost_HPO... ‚úì MSE=0.3570 (0.2s)\n",
      "  üîÑ LGD_mlp_DEFAULT... ‚úì MSE=0.3070 (2.2s)\n",
      "  üîÑ LGD_mlp_HPO... ‚úì MSE=0.4904 (4.0s)\n",
      "  üîÑ LGD_tabnet_DEFAULT... ‚úì MSE=0.3294 (9.3s)\n",
      "  üîÑ LGD_tabnet_HPO... ‚úì MSE=0.4670 (24.3s)\n",
      "  üîÑ LGD_PFN-v2_DEFAULT... ‚úì MSE=0.3935 (1.3s)\n",
      "  ‚äò LGD_PFN-v2_HPO... Skipped (no HPO support)\n",
      "\n",
      "================================================================================\n",
      " COMPILING RESULTS\n",
      "================================================================================\n",
      "\n",
      "üìà Overall Summary:\n",
      "  Total tests: 42\n",
      "  Successful: 42\n",
      "  Failed: 0\n",
      "  Success rate: 100.0%\n",
      "\n",
      "üìä Results by Task:\n",
      "  PD: 26/26 successful\n",
      "  LGD: 16/16 successful\n",
      "\n",
      "================================================================================\n",
      " HPO IMPACT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üîç HPO Impact Summary:\n",
      "  Methods with improvement: 10/18\n",
      "  Average improvement: -2.61%\n",
      "  Max improvement: 26.32%\n",
      "  Min improvement: -59.74%\n",
      "\n",
      "üèÜ Top 5 Improvements with HPO:\n",
      "--------------------------------------------------------------------------------\n",
      "  LGD | knn                  | MSE     : 0.4124 ‚Üí 0.3039 (+26.32%)\n",
      "  PD  | knn                  | AUC     : 0.6176 ‚Üí 0.7549 (+22.24%)\n",
      "  PD  | LogReg               | AUC     : 0.7311 ‚Üí 0.7983 (+9.18%)\n",
      "  PD  | dcn2                 | AUC     : 0.5595 ‚Üí 0.5973 (+6.76%)\n",
      "  PD  | xgboost              | AUC     : 0.7315 ‚Üí 0.7759 (+6.07%)\n",
      "\n",
      "‚ö† Methods Where HPO Decreased Performance:\n",
      "--------------------------------------------------------------------------------\n",
      "  PD  | catboost             | AUC     : 0.8119 ‚Üí 0.8037 (-1.01%)\n",
      "  PD  | mlp                  | AUC     : 0.7675 ‚Üí 0.6930 (-9.70%)\n",
      "  PD  | snn                  | AUC     : 0.6986 ‚Üí 0.5829 (-16.56%)\n",
      "  LGD | catboost             | MSE     : 0.3586 ‚Üí 0.3663 (-2.15%)\n",
      "  LGD | RandomForest         | MSE     : 0.3357 ‚Üí 0.3374 (-0.52%)\n",
      "  LGD | mlp                  | MSE     : 0.3070 ‚Üí 0.4904 (-59.74%)\n",
      "  LGD | tabnet               | MSE     : 0.3294 ‚Üí 0.4670 (-41.77%)\n",
      "\n",
      "================================================================================\n",
      " DETAILED RESULTS\n",
      "================================================================================\n",
      "\n",
      "PD Results:\n",
      "--------------------------------------------------------------------------------\n",
      "      Method HPO Performance  Time  Folds\n",
      "    catboost  No  AUC=0.8119  5.5s      3\n",
      "    catboost Yes  AUC=0.8037  2.3s      3\n",
      "         knn  No  AUC=0.6176  0.1s      3\n",
      "         knn Yes  AUC=0.7549  0.1s      3\n",
      "    lightgbm  No  AUC=0.7402  3.6s      3\n",
      "    lightgbm Yes  AUC=0.7526  0.2s      3\n",
      "      LogReg  No  AUC=0.7311  0.1s      3\n",
      "      LogReg Yes  AUC=0.7983  0.1s      3\n",
      "  NaiveBayes  No  AUC=0.6495  0.1s      3\n",
      "RandomForest  No  AUC=0.8065  5.2s      3\n",
      "RandomForest Yes  AUC=0.8213  0.3s      3\n",
      "         svm  No  AUC=0.5061  0.1s      3\n",
      "         svm Yes  AUC=0.5061  0.1s      3\n",
      "     xgboost  No  AUC=0.7315  1.0s      3\n",
      "     xgboost Yes  AUC=0.7759  0.2s      3\n",
      "         NCM  No  AUC=0.6849  0.2s      3\n",
      "       dummy  No  AUC=0.5000  0.1s      3\n",
      "         mlp  No  AUC=0.7675  4.1s      3\n",
      "         mlp Yes  AUC=0.6930  4.1s      3\n",
      "      tabnet  No  AUC=0.4695  5.2s      3\n",
      "      tabnet Yes  AUC=0.4939 16.4s      3\n",
      "      tabpfn  No  AUC=0.8081  0.6s      3\n",
      "         snn  No  AUC=0.6986  3.0s      3\n",
      "         snn Yes  AUC=0.5829  4.7s      3\n",
      "        dcn2  No  AUC=0.5595  4.8s      3\n",
      "        dcn2 Yes  AUC=0.5973  2.4s      3\n",
      "\n",
      "LGD Results:\n",
      "--------------------------------------------------------------------------------\n",
      "          Method HPO Performance  Time  Folds\n",
      "        catboost  No  MSE=0.3586  3.8s      3\n",
      "        catboost Yes  MSE=0.3663  2.7s      3\n",
      "             knn  No  MSE=0.4124  0.1s      3\n",
      "             knn Yes  MSE=0.3039  0.1s      3\n",
      "        lightgbm  No  MSE=0.3898  3.7s      3\n",
      "        lightgbm Yes  MSE=0.3875  0.5s      3\n",
      "LinearRegression  No  MSE=0.3334  0.0s      3\n",
      "    RandomForest  No  MSE=0.3357  5.8s      3\n",
      "    RandomForest Yes  MSE=0.3374  0.3s      3\n",
      "         xgboost  No  MSE=0.3741  0.3s      3\n",
      "         xgboost Yes  MSE=0.3570  0.2s      3\n",
      "             mlp  No  MSE=0.3070  2.2s      3\n",
      "             mlp Yes  MSE=0.4904  4.0s      3\n",
      "          tabnet  No  MSE=0.3294  9.3s      3\n",
      "          tabnet Yes  MSE=0.4670 24.3s      3\n",
      "          PFN-v2  No  MSE=0.3935  1.3s      3\n",
      "\n",
      "================================================================================\n",
      " DEBUGGING INFORMATION\n",
      "================================================================================\n",
      "\n",
      "üîç Configuration Used:\n",
      "  Row limit: 1000\n",
      "  CV splits: 3\n",
      "  HPO trials: 100\n",
      "  Max epochs: 50\n",
      "  Verbose: False\n",
      "\n",
      "üîç Methods Tested:\n",
      "  PD:\n",
      "    Classical: ['catboost', 'knn', 'lightgbm', 'LogReg', 'NaiveBayes', 'RandomForest', 'svm', 'xgboost', 'NCM', 'dummy']\n",
      "    Deep: ['mlp', 'tabnet', 'tabpfn', 'snn', 'dcn2']\n",
      "  LGD:\n",
      "    Classical: ['catboost', 'knn', 'lightgbm', 'LinearRegression', 'RandomForest', 'xgboost']\n",
      "    Deep: ['mlp', 'tabnet', 'PFN-v2']\n",
      "\n",
      "üîç Timing Statistics:\n",
      "  Average time per test: 2.9s\n",
      "  Total time: 2.1m\n",
      "  Fastest test: 0.0s (LinearRegression)\n",
      "  Slowest test: 24.3s (tabnet)\n",
      "\n",
      "üîç Error Analysis:\n",
      "  ‚úÖ No errors occurred!\n",
      "\n",
      "================================================================================\n",
      " EXPORT\n",
      "================================================================================\n",
      "\n",
      "üíæ Results saved to: c:\\Users\\U0152019\\OneDrive - KU Leuven\\PhD Documents\\Projects\\1. TabPFN\\TabPFNCredit\\results\\method_test_results_20251113_141636.csv\n",
      "\n",
      "================================================================================\n",
      " TEST COMPLETE\n",
      "================================================================================\n",
      "\n",
      "End time: 2025-11-13 14:16:36\n",
      "Total tests: 42\n",
      "Success rate: 100.0%\n",
      "\n",
      "‚úÖ Testing complete! Check the summary above for details.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=================================================================================\n",
    "COMPREHENSIVE METHOD TESTING SCRIPT (FIXED)\n",
    "=================================================================================\n",
    "Tests all TALENT methods on PD and LGD datasets with and without HPO.\n",
    "Provides detailed debugging output and performance comparisons.\n",
    "=================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.methods.method_runner import (\n",
    "    run_talent_method,\n",
    "    get_available_methods,\n",
    "    supports_hpo,\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SECTION - MODIFY THESE PARAMETERS EASILY\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Dataset configuration\n",
    "    'datasets': {\n",
    "        'pd': '0001.gmsc',      # First PD dataset\n",
    "        'lgd': '0001.heloc',    # First LGD dataset\n",
    "    },\n",
    "    \n",
    "    # Method selection - ONLY test these specific methods\n",
    "    'methods_to_test': {\n",
    "        'pd': {\n",
    "            'classical': ['catboost', 'knn', 'lightgbm', 'LogReg', 'NaiveBayes', \n",
    "                         'RandomForest', 'svm', 'xgboost', 'NCM', 'dummy'],\n",
    "            'deep': ['mlp', 'tabnet', 'tabpfn','snn','dcn2'],\n",
    "        },\n",
    "        'lgd': {\n",
    "            'classical': ['catboost', 'knn', 'lightgbm', 'LinearRegression', \n",
    "                         'RandomForest', 'xgboost'],\n",
    "            'deep': ['mlp', 'tabnet', 'PFN-v2'],\n",
    "        },\n",
    "    },\n",
    "    \n",
    "    # Data split configuration\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.2,\n",
    "    'cv_splits': 3,\n",
    "    'seed': 42,\n",
    "    'row_limit': 1000,          # Small for fast testing\n",
    "    'sampling': None,\n",
    "    \n",
    "    # Training configuration\n",
    "    'max_epoch': 50,            # Reduced for speed\n",
    "    'batch_size': 256,          # Smaller batch for small data\n",
    "    'early_stopping': True,\n",
    "    'early_stopping_patience': 10,\n",
    "    'evaluate_option': 'best-val',\n",
    "    \n",
    "    # HPO configuration\n",
    "    'n_trials': 100,             # Reduced for speed (normally 100)\n",
    "    'run_with_hpo': True,       # Test with HPO\n",
    "    'run_without_hpo': True,    # Test without HPO\n",
    "    \n",
    "    # Preprocessing (None = use defaults)\n",
    "    'categorical_encoding': None,\n",
    "    'numerical_encoding': None,\n",
    "    'normalization': None,\n",
    "    'num_nan_policy': None,\n",
    "    'cat_nan_policy': None,\n",
    "    \n",
    "    # Execution configuration\n",
    "    'verbose': False,\n",
    "    'clean_temp_dir': True,\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def format_time(seconds):\n",
    "    \"\"\"Format seconds into human-readable time.\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f}s\"\n",
    "    elif seconds < 3600:\n",
    "        return f\"{seconds/60:.1f}m\"\n",
    "    else:\n",
    "        return f\"{seconds/3600:.1f}h\"\n",
    "\n",
    "\n",
    "def compute_metrics(y_true, y_pred, task):\n",
    "    \"\"\"Compute standard metrics for comparison.\"\"\"\n",
    "    from sklearn.metrics import (\n",
    "        roc_auc_score, accuracy_score, f1_score,\n",
    "        mean_squared_error, r2_score, mean_absolute_error\n",
    "    )\n",
    "    \n",
    "    if task == 'pd':  # Classification\n",
    "        # Handle probability predictions\n",
    "        if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "            y_pred_proba = y_pred[:, 1]  # Positive class probability\n",
    "            y_pred_class = np.argmax(y_pred, axis=1)\n",
    "        else:\n",
    "            y_pred_proba = y_pred\n",
    "            y_pred_class = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        except:\n",
    "            auc = np.nan\n",
    "        \n",
    "        try:\n",
    "            acc = accuracy_score(y_true, y_pred_class)\n",
    "        except:\n",
    "            acc = np.nan\n",
    "        \n",
    "        try:\n",
    "            f1 = f1_score(y_true, y_pred_class, average='binary')\n",
    "        except:\n",
    "            f1 = np.nan\n",
    "        \n",
    "        return {'AUC': auc, 'Accuracy': acc, 'F1': f1}\n",
    "    \n",
    "    else:  # Regression\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R¬≤': r2}\n",
    "\n",
    "\n",
    "def aggregate_cv_metrics(results, task):\n",
    "    \"\"\"Aggregate metrics across CV folds.\"\"\"\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    for fold_id, fold_result in results.items():\n",
    "        all_y_true.append(fold_result['y_true'])\n",
    "        all_y_pred.append(fold_result['y_pred'])\n",
    "    \n",
    "    # Concatenate all folds\n",
    "    y_true_all = np.concatenate(all_y_true)\n",
    "    y_pred_all = np.concatenate(all_y_pred)\n",
    "    \n",
    "    # Compute metrics on all folds combined\n",
    "    return compute_metrics(y_true_all, y_pred_all, task)\n",
    "\n",
    "\n",
    "def run_single_test(task, dataset, method, use_hpo, config):\n",
    "    \"\"\"\n",
    "    Run a single method test with comprehensive error handling.\n",
    "    \n",
    "    Returns:\n",
    "        dict with test results or error information\n",
    "    \"\"\"\n",
    "    test_name = f\"{task.upper()}_{method}_{'HPO' if use_hpo else 'DEFAULT'}\"\n",
    "    \n",
    "    result = {\n",
    "        'task': task.upper(),\n",
    "        'dataset': dataset,\n",
    "        'method': method,\n",
    "        'hpo': use_hpo,\n",
    "        'status': 'PENDING',\n",
    "        'error': None,\n",
    "        'metrics': {},\n",
    "        'total_time': 0,\n",
    "        'avg_fold_time': 0,\n",
    "        'n_folds': 0,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(f\"  üîÑ {test_name}...\", end=\" \", flush=True)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run method\n",
    "        results = run_talent_method(\n",
    "            task=task,\n",
    "            dataset=dataset,\n",
    "            test_size=config['test_size'],\n",
    "            val_size=config['val_size'],\n",
    "            cv_splits=config['cv_splits'],\n",
    "            seed=config['seed'],\n",
    "            row_limit=config['row_limit'],\n",
    "            sampling=config['sampling'],\n",
    "            method=method,\n",
    "            categorical_encoding=config['categorical_encoding'],\n",
    "            numerical_encoding=config['numerical_encoding'],\n",
    "            normalization=config['normalization'],\n",
    "            num_nan_policy=config['num_nan_policy'],\n",
    "            cat_nan_policy=config['cat_nan_policy'],\n",
    "            max_epoch=config['max_epoch'],\n",
    "            batch_size=config['batch_size'],\n",
    "            tune=use_hpo,\n",
    "            n_trials=config['n_trials'],\n",
    "            early_stopping=config['early_stopping'],\n",
    "            early_stopping_patience=config['early_stopping_patience'],\n",
    "            evaluate_option=config['evaluate_option'],\n",
    "            verbose=config['verbose'],\n",
    "            clean_temp_dir=config['clean_temp_dir'],\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Aggregate metrics\n",
    "        agg_metrics = aggregate_cv_metrics(results, task)\n",
    "        \n",
    "        # Compute average training time\n",
    "        fold_times = [r['train_time'] for r in results.values()]\n",
    "        avg_time = np.mean(fold_times)\n",
    "        \n",
    "        # Update result\n",
    "        result.update({\n",
    "            'status': 'SUCCESS',\n",
    "            'metrics': agg_metrics,\n",
    "            'total_time': elapsed,\n",
    "            'avg_fold_time': avg_time,\n",
    "            'n_folds': len(results),\n",
    "        })\n",
    "        \n",
    "        # Print success with primary metric\n",
    "        primary_metric = list(agg_metrics.keys())[0]\n",
    "        primary_value = agg_metrics[primary_metric]\n",
    "        print(f\"‚úì {primary_metric}={primary_value:.4f} ({format_time(elapsed)})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_msg = str(e)[:100]  # Truncate long errors\n",
    "        \n",
    "        result.update({\n",
    "            'status': 'FAILED',\n",
    "            'error': error_msg,\n",
    "            'total_time': elapsed,\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úó {error_msg[:50]}... ({format_time(elapsed)})\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TESTING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\" COMPREHENSIVE TALENT METHOD TESTING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nStart time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Print Configuration\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\nüìã Configuration:\")\n",
    "    print(f\"  Datasets:\")\n",
    "    print(f\"    PD:  {CONFIG['datasets']['pd']}\")\n",
    "    print(f\"    LGD: {CONFIG['datasets']['lgd']}\")\n",
    "    print(f\"  Data:\")\n",
    "    print(f\"    Row limit: {CONFIG['row_limit']}\")\n",
    "    print(f\"    CV splits: {CONFIG['cv_splits']}\")\n",
    "    print(f\"    Test/Val size: {CONFIG['test_size']}/{CONFIG['val_size']}\")\n",
    "    print(f\"  Training:\")\n",
    "    print(f\"    Max epochs: {CONFIG['max_epoch']}\")\n",
    "    print(f\"    Batch size: {CONFIG['batch_size']}\")\n",
    "    print(f\"  HPO:\")\n",
    "    print(f\"    Trials: {CONFIG['n_trials']}\")\n",
    "    print(f\"    Run with HPO: {CONFIG['run_with_hpo']}\")\n",
    "    print(f\"    Run without HPO: {CONFIG['run_without_hpo']}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Display Methods to Test\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(f\"\\nüìä Methods to test:\")\n",
    "    for task in ['pd', 'lgd']:\n",
    "        task_methods = CONFIG['methods_to_test'][task]\n",
    "        total = len(task_methods['classical']) + len(task_methods['deep'])\n",
    "        print(f\"  {task.upper()}: {total} methods\")\n",
    "        print(f\"    Classical: {', '.join(task_methods['classical'])}\")\n",
    "        print(f\"    Deep: {', '.join(task_methods['deep'])}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Run Tests\n",
    "    # -------------------------------------------------------------------------\n",
    "    all_results = []\n",
    "    \n",
    "    for task_type in ['pd', 'lgd']:\n",
    "        dataset = CONFIG['datasets'][task_type]\n",
    "        \n",
    "        # Get methods for this task\n",
    "        methods = (CONFIG['methods_to_test'][task_type]['classical'] + \n",
    "                  CONFIG['methods_to_test'][task_type]['deep'])\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\" TESTING {task_type.upper()} DATASET: {dataset}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Testing {len(methods)} methods...\")\n",
    "        \n",
    "        for method in methods:\n",
    "            # Check if method supports HPO\n",
    "            method_supports_hpo = supports_hpo(method)\n",
    "            \n",
    "            # Test without HPO\n",
    "            if CONFIG['run_without_hpo']:\n",
    "                result = run_single_test(\n",
    "                    task=task_type,\n",
    "                    dataset=dataset,\n",
    "                    method=method,\n",
    "                    use_hpo=False,\n",
    "                    config=CONFIG\n",
    "                )\n",
    "                all_results.append(result)\n",
    "            \n",
    "            # Test with HPO (if supported)\n",
    "            if CONFIG['run_with_hpo'] and method_supports_hpo:\n",
    "                result = run_single_test(\n",
    "                    task=task_type,\n",
    "                    dataset=dataset,\n",
    "                    method=method,\n",
    "                    use_hpo=True,\n",
    "                    config=CONFIG\n",
    "                )\n",
    "                all_results.append(result)\n",
    "            elif CONFIG['run_with_hpo'] and not method_supports_hpo:\n",
    "                print(f\"  ‚äò {task_type.upper()}_{method}_HPO... Skipped (no HPO support)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Create Results DataFrame\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" COMPILING RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Summary Statistics\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\nüìà Overall Summary:\")\n",
    "    print(f\"  Total tests: {len(df)}\")\n",
    "    print(f\"  Successful: {len(df[df['status'] == 'SUCCESS'])}\")\n",
    "    print(f\"  Failed: {len(df[df['status'] == 'FAILED'])}\")\n",
    "    if len(df) > 0:\n",
    "        print(f\"  Success rate: {(len(df[df['status'] == 'SUCCESS']) / len(df) * 100):.1f}%\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Results by Task\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"\\nüìä Results by Task:\")\n",
    "    for task in ['PD', 'LGD']:\n",
    "        task_df = df[df['task'] == task]\n",
    "        if len(task_df) > 0:\n",
    "            success_count = len(task_df[task_df['status'] == 'SUCCESS'])\n",
    "            print(f\"  {task}: {success_count}/{len(task_df)} successful\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Failed Tests Detail\n",
    "    # -------------------------------------------------------------------------\n",
    "    failed_df = df[df['status'] == 'FAILED']\n",
    "    if len(failed_df) > 0:\n",
    "        print(f\"\\n‚ùå Failed Tests ({len(failed_df)}):\")\n",
    "        print(\"-\"*80)\n",
    "        for _, row in failed_df.iterrows():\n",
    "            hpo_str = \"with HPO\" if row['hpo'] else \"default\"\n",
    "            print(f\"  {row['task']:3s} | {row['method']:20s} | {hpo_str:10s} | {row['error']}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Performance Comparison: HPO vs No HPO\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" HPO IMPACT ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Only for methods tested both ways\n",
    "    success_df = df[df['status'] == 'SUCCESS'].copy()\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    if len(success_df) > 0:\n",
    "        for task in ['PD', 'LGD']:\n",
    "            task_df = success_df[success_df['task'] == task]\n",
    "            \n",
    "            for method in task_df['method'].unique():\n",
    "                method_df = task_df[task_df['method'] == method]\n",
    "                \n",
    "                # Check if we have both HPO and non-HPO results\n",
    "                has_hpo = len(method_df[method_df['hpo'] == True]) > 0\n",
    "                has_no_hpo = len(method_df[method_df['hpo'] == False]) > 0\n",
    "                \n",
    "                if has_hpo and has_no_hpo:\n",
    "                    hpo_row = method_df[method_df['hpo'] == True].iloc[0]\n",
    "                    no_hpo_row = method_df[method_df['hpo'] == False].iloc[0]\n",
    "                    \n",
    "                    # Get primary metric\n",
    "                    primary_metric = list(hpo_row['metrics'].keys())[0]\n",
    "                    hpo_value = hpo_row['metrics'][primary_metric]\n",
    "                    no_hpo_value = no_hpo_row['metrics'][primary_metric]\n",
    "                    \n",
    "                    # For classification (higher is better), for regression (depends on metric)\n",
    "                    if task == 'PD':  # Higher is better\n",
    "                        improvement = hpo_value - no_hpo_value\n",
    "                        improvement_pct = (improvement / no_hpo_value) * 100 if no_hpo_value != 0 else 0\n",
    "                    else:  # Regression\n",
    "                        if primary_metric == 'R¬≤':  # Higher is better\n",
    "                            improvement = hpo_value - no_hpo_value\n",
    "                            improvement_pct = (improvement / no_hpo_value) * 100 if no_hpo_value != 0 else 0\n",
    "                        else:  # Lower is better (error metrics)\n",
    "                            improvement = no_hpo_value - hpo_value\n",
    "                            improvement_pct = (improvement / no_hpo_value) * 100 if no_hpo_value != 0 else 0\n",
    "                    \n",
    "                    comparison_results.append({\n",
    "                        'task': task,\n",
    "                        'method': method,\n",
    "                        'metric': primary_metric,\n",
    "                        'no_hpo': no_hpo_value,\n",
    "                        'hpo': hpo_value,\n",
    "                        'improvement': improvement,\n",
    "                        'improvement_pct': improvement_pct,\n",
    "                        'time_no_hpo': no_hpo_row['total_time'],\n",
    "                        'time_hpo': hpo_row['total_time'],\n",
    "                    })\n",
    "        \n",
    "        if comparison_results:\n",
    "            comp_df = pd.DataFrame(comparison_results)\n",
    "            \n",
    "            print(\"\\nüîç HPO Impact Summary:\")\n",
    "            print(f\"  Methods with improvement: {len(comp_df[comp_df['improvement'] > 0])}/{len(comp_df)}\")\n",
    "            print(f\"  Average improvement: {comp_df['improvement_pct'].mean():.2f}%\")\n",
    "            print(f\"  Max improvement: {comp_df['improvement_pct'].max():.2f}%\")\n",
    "            print(f\"  Min improvement: {comp_df['improvement_pct'].min():.2f}%\")\n",
    "            \n",
    "            # Top improvements\n",
    "            print(\"\\nüèÜ Top 5 Improvements with HPO:\")\n",
    "            print(\"-\"*80)\n",
    "            top_5 = comp_df.nlargest(min(5, len(comp_df)), 'improvement_pct')\n",
    "            for _, row in top_5.iterrows():\n",
    "                print(f\"  {row['task']:3s} | {row['method']:20s} | \"\n",
    "                      f\"{row['metric']:8s}: {row['no_hpo']:.4f} ‚Üí {row['hpo']:.4f} \"\n",
    "                      f\"({row['improvement_pct']:+.2f}%)\")\n",
    "            \n",
    "            # Methods where HPO hurt performance\n",
    "            hurt_df = comp_df[comp_df['improvement'] < 0]\n",
    "            if len(hurt_df) > 0:\n",
    "                print(\"\\n‚ö† Methods Where HPO Decreased Performance:\")\n",
    "                print(\"-\"*80)\n",
    "                for _, row in hurt_df.iterrows():\n",
    "                    print(f\"  {row['task']:3s} | {row['method']:20s} | \"\n",
    "                          f\"{row['metric']:8s}: {row['no_hpo']:.4f} ‚Üí {row['hpo']:.4f} \"\n",
    "                          f\"({row['improvement_pct']:.2f}%)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Detailed Results Table\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" DETAILED RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if len(success_df) > 0:\n",
    "        # Create display dataframe\n",
    "        display_df = success_df.copy()\n",
    "        \n",
    "        # Extract primary metric values\n",
    "        display_df['primary_metric_name'] = display_df['metrics'].apply(\n",
    "            lambda x: list(x.keys())[0] if x else 'N/A'\n",
    "        )\n",
    "        display_df['primary_metric_value'] = display_df['metrics'].apply(\n",
    "            lambda x: list(x.values())[0] if x else np.nan\n",
    "        )\n",
    "        \n",
    "        # Format for display\n",
    "        display_df['HPO'] = display_df['hpo'].map({True: 'Yes', False: 'No'})\n",
    "        display_df['Time'] = display_df['total_time'].apply(format_time)\n",
    "        display_df['Metric'] = display_df.apply(\n",
    "            lambda r: f\"{r['primary_metric_name']}={r['primary_metric_value']:.4f}\", axis=1\n",
    "        )\n",
    "        \n",
    "        # Create final table\n",
    "        final_table = display_df[[\n",
    "            'task', 'method', 'HPO', 'Metric', 'Time', 'n_folds'\n",
    "        ]].copy()\n",
    "        \n",
    "        final_table.columns = ['Task', 'Method', 'HPO', 'Performance', 'Time', 'Folds']\n",
    "        \n",
    "        # Print by task\n",
    "        for task in ['PD', 'LGD']:\n",
    "            task_table = final_table[final_table['Task'] == task]\n",
    "            if len(task_table) > 0:\n",
    "                print(f\"\\n{task} Results:\")\n",
    "                print(\"-\"*80)\n",
    "                task_table_display = task_table.drop('Task', axis=1)\n",
    "                print(task_table_display.to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\n‚ö† No successful tests to display.\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Debugging Information\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" DEBUGGING INFORMATION\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(\"\\nüîç Configuration Used:\")\n",
    "    print(f\"  Row limit: {CONFIG['row_limit']}\")\n",
    "    print(f\"  CV splits: {CONFIG['cv_splits']}\")\n",
    "    print(f\"  HPO trials: {CONFIG['n_trials']}\")\n",
    "    print(f\"  Max epochs: {CONFIG['max_epoch']}\")\n",
    "    print(f\"  Verbose: {CONFIG['verbose']}\")\n",
    "    \n",
    "    print(\"\\nüîç Methods Tested:\")\n",
    "    for task in ['pd', 'lgd']:\n",
    "        print(f\"  {task.upper()}:\")\n",
    "        print(f\"    Classical: {CONFIG['methods_to_test'][task]['classical']}\")\n",
    "        print(f\"    Deep: {CONFIG['methods_to_test'][task]['deep']}\")\n",
    "    \n",
    "    print(\"\\nüîç Timing Statistics:\")\n",
    "    if len(success_df) > 0:\n",
    "        print(f\"  Average time per test: {format_time(success_df['total_time'].mean())}\")\n",
    "        print(f\"  Total time: {format_time(success_df['total_time'].sum())}\")\n",
    "        print(f\"  Fastest test: {format_time(success_df['total_time'].min())} \"\n",
    "              f\"({success_df.loc[success_df['total_time'].idxmin(), 'method']})\")\n",
    "        print(f\"  Slowest test: {format_time(success_df['total_time'].max())} \"\n",
    "              f\"({success_df.loc[success_df['total_time'].idxmax(), 'method']})\")\n",
    "    else:\n",
    "        print(\"  No successful tests to analyze.\")\n",
    "    \n",
    "    print(\"\\nüîç Error Analysis:\")\n",
    "    if len(failed_df) > 0:\n",
    "        error_counts = failed_df['error'].value_counts()\n",
    "        print(\"  Most common errors:\")\n",
    "        for error, count in error_counts.head(5).items():\n",
    "            print(f\"    - {error[:60]}... ({count} occurrences)\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ No errors occurred!\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Export Results\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" EXPORT\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    output_dir = PROJECT_ROOT / \"results\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_file = output_dir / f\"method_test_results_{timestamp}.csv\"\n",
    "    \n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nüíæ Results saved to: {output_file}\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Final Summary\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\" TEST COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total tests: {len(df)}\")\n",
    "    if len(df) > 0:\n",
    "        print(f\"Success rate: {(len(success_df) / len(df) * 100):.1f}%\")\n",
    "    print(f\"\\n‚úÖ Testing complete! Check the summary above for details.\")\n",
    "    \n",
    "    return df, success_df, comparison_results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# RUN THE TESTS\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_df, success_df, hpo_comparison = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
