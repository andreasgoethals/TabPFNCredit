{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484285d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE METHOD BENCHMARKING SCRIPT\n",
    "# Runs all classical + selected deep methods on all datasets\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.methods.method_runner import run_talent_method, get_available_methods\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Get all available datasets\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Scan for PD and LGD datasets\n",
    "PD_DATASETS = sorted([d.name for d in (DATA_DIR / \"pd\").glob(\"*.csv\")]) if (DATA_DIR / \"pd\").exists() else []\n",
    "LGD_DATASETS = sorted([d.name for d in (DATA_DIR / \"lgd\").glob(\"*.csv\")]) if (DATA_DIR / \"lgd\").exists() else []\n",
    "\n",
    "# Remove .csv extension\n",
    "PD_DATASETS = [d.replace('.csv', '') for d in PD_DATASETS]\n",
    "LGD_DATASETS = [d.replace('.csv', '') for d in LGD_DATASETS]\n",
    "\n",
    "print(f\"Found {len(PD_DATASETS)} PD datasets: {PD_DATASETS[:3]}...\")\n",
    "print(f\"Found {len(LGD_DATASETS)} LGD datasets: {LGD_DATASETS[:3]}...\")\n",
    "\n",
    "# Methods to benchmark\n",
    "CLASSIFICATION_METHODS = [\n",
    "    # Classical (fast)\n",
    "    'xgboost', 'catboost', 'lightgbm', 'RandomForest',\n",
    "    # Deep (slower)\n",
    "    'tabpfn', 'tabnet', 'mlp'\n",
    "]\n",
    "\n",
    "REGRESSION_METHODS = [\n",
    "    # Classical (fast)\n",
    "    'xgboost', 'catboost', 'lightgbm', 'RandomForest', \n",
    "    # Deep (slower)\n",
    "    'tabnet', 'mlp'  # TabPFN doesn't support regression well\n",
    "]\n",
    "\n",
    "# Benchmark settings\n",
    "CV_SPLITS = 3\n",
    "SEED = 42\n",
    "ROW_LIMIT = None  # Set to e.g., 1000 for quick testing\n",
    "TUNE_HPO = False  # Set True to enable HPO (will be much slower!)\n",
    "N_TRIALS = 10 if TUNE_HPO else 0\n",
    "\n",
    "print(f\"\\nBenchmark configuration:\")\n",
    "print(f\"  CV splits: {CV_SPLITS}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Row limit: {ROW_LIMIT if ROW_LIMIT else 'None (full datasets)'}\")\n",
    "print(f\"  HPO enabled: {TUNE_HPO}\")\n",
    "if TUNE_HPO:\n",
    "    print(f\"  HPO trials: {N_TRIALS}\")\n",
    "\n",
    "# =============================================================================\n",
    "# BENCHMARKING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# Total number of experiments\n",
    "n_pd_experiments = len(PD_DATASETS) * len(CLASSIFICATION_METHODS)\n",
    "n_lgd_experiments = len(LGD_DATASETS) * len(REGRESSION_METHODS)\n",
    "total_experiments = n_pd_experiments + n_lgd_experiments\n",
    "\n",
    "print(f\"\\nTotal experiments to run: {total_experiments}\")\n",
    "print(f\"  PD: {len(PD_DATASETS)} datasets Ã— {len(CLASSIFICATION_METHODS)} methods = {n_pd_experiments}\")\n",
    "print(f\"  LGD: {len(LGD_DATASETS)} datasets Ã— {len(REGRESSION_METHODS)} methods = {n_lgd_experiments}\")\n",
    "\n",
    "# Create progress bar\n",
    "pbar = tqdm(total=total_experiments, desc=\"Benchmarking\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# PD (Classification) Datasets\n",
    "# -------------------------------------------------------------------------\n",
    "for dataset in PD_DATASETS:\n",
    "    for method in CLASSIFICATION_METHODS:\n",
    "        try:\n",
    "            # Run method\n",
    "            fold_results = run_talent_method(\n",
    "                task='pd',\n",
    "                dataset=dataset,\n",
    "                test_size=0.2,\n",
    "                val_size=0.2,\n",
    "                cv_splits=CV_SPLITS,\n",
    "                seed=SEED,\n",
    "                row_limit=ROW_LIMIT,\n",
    "                method=method,\n",
    "                tune=TUNE_HPO,\n",
    "                n_trials=N_TRIALS,\n",
    "                verbose=False,  # Silent mode\n",
    "                clean_temp_dir=True,\n",
    "            )\n",
    "            \n",
    "            # Extract accuracy (first metric) from each fold\n",
    "            accuracies = [fold_results[fold_id]['metrics'][0] \n",
    "                         for fold_id in sorted(fold_results.keys())]\n",
    "            avg_accuracy = np.mean(accuracies)\n",
    "            std_accuracy = np.std(accuracies)\n",
    "            \n",
    "            # Store result\n",
    "            results_list.append({\n",
    "                'Task': 'PD',\n",
    "                'Dataset': dataset,\n",
    "                'Method': method,\n",
    "                'Avg_Accuracy': avg_accuracy,\n",
    "                'Std_Accuracy': std_accuracy,\n",
    "                'Folds': CV_SPLITS,\n",
    "                'HPO': TUNE_HPO,\n",
    "            })\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Dataset': dataset[:15],\n",
    "                'Method': method,\n",
    "                'Acc': f\"{avg_accuracy:.4f}\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâœ— Error: {dataset} + {method}: {e}\")\n",
    "            results_list.append({\n",
    "                'Task': 'PD',\n",
    "                'Dataset': dataset,\n",
    "                'Method': method,\n",
    "                'Avg_Accuracy': np.nan,\n",
    "                'Std_Accuracy': np.nan,\n",
    "                'Folds': CV_SPLITS,\n",
    "                'HPO': TUNE_HPO,\n",
    "                'Error': str(e)\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# LGD (Regression) Datasets\n",
    "# -------------------------------------------------------------------------\n",
    "for dataset in LGD_DATASETS:\n",
    "    for method in REGRESSION_METHODS:\n",
    "        try:\n",
    "            # Run method\n",
    "            fold_results = run_talent_method(\n",
    "                task='lgd',\n",
    "                dataset=dataset,\n",
    "                test_size=0.2,\n",
    "                val_size=0.2,\n",
    "                cv_splits=CV_SPLITS,\n",
    "                seed=SEED,\n",
    "                row_limit=ROW_LIMIT,\n",
    "                method=method,\n",
    "                tune=TUNE_HPO,\n",
    "                n_trials=N_TRIALS,\n",
    "                verbose=False,\n",
    "                clean_temp_dir=True,\n",
    "            )\n",
    "            \n",
    "            # Extract R2 (or first metric) from each fold\n",
    "            metric_values = [fold_results[fold_id]['metrics'][0] \n",
    "                            for fold_id in sorted(fold_results.keys())]\n",
    "            avg_metric = np.mean(metric_values)\n",
    "            std_metric = np.std(metric_values)\n",
    "            \n",
    "            # Get metric name (usually R2 for regression)\n",
    "            metric_name = fold_results[min(fold_results.keys())]['primary_metric']\n",
    "            \n",
    "            # Store result\n",
    "            results_list.append({\n",
    "                'Task': 'LGD',\n",
    "                'Dataset': dataset,\n",
    "                'Method': method,\n",
    "                'Avg_Metric': avg_metric,\n",
    "                'Std_Metric': std_metric,\n",
    "                'Metric_Name': metric_name,\n",
    "                'Folds': CV_SPLITS,\n",
    "                'HPO': TUNE_HPO,\n",
    "            })\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Dataset': dataset[:15],\n",
    "                'Method': method,\n",
    "                metric_name: f\"{avg_metric:.4f}\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâœ— Error: {dataset} + {method}: {e}\")\n",
    "            results_list.append({\n",
    "                'Task': 'LGD',\n",
    "                'Dataset': dataset,\n",
    "                'Method': method,\n",
    "                'Avg_Metric': np.nan,\n",
    "                'Std_Metric': np.nan,\n",
    "                'Metric_Name': 'Unknown',\n",
    "                'Folds': CV_SPLITS,\n",
    "                'HPO': TUNE_HPO,\n",
    "                'Error': str(e)\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" BENCHMARK RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display PD results\n",
    "print(\"\\nðŸ“Š PD (Classification) Results:\")\n",
    "print(\"-\" * 80)\n",
    "if 'Avg_Accuracy' in df.columns:\n",
    "    pd_results = df[df['Task'] == 'PD'][['Dataset', 'Method', 'Avg_Accuracy', 'Std_Accuracy']]\n",
    "    pd_results = pd_results.sort_values(['Dataset', 'Avg_Accuracy'], ascending=[True, False])\n",
    "    print(pd_results.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ PD Summary Statistics:\")\n",
    "    print(f\"  Best average accuracy: {pd_results['Avg_Accuracy'].max():.4f}\")\n",
    "    print(f\"  Worst average accuracy: {pd_results['Avg_Accuracy'].min():.4f}\")\n",
    "    print(f\"  Mean across all: {pd_results['Avg_Accuracy'].mean():.4f}\")\n",
    "\n",
    "# Display LGD results\n",
    "print(\"\\nðŸ“Š LGD (Regression) Results:\")\n",
    "print(\"-\" * 80)\n",
    "if 'Avg_Metric' in df.columns:\n",
    "    lgd_results = df[df['Task'] == 'LGD'][['Dataset', 'Method', 'Avg_Metric', 'Std_Metric', 'Metric_Name']]\n",
    "    lgd_results = lgd_results.sort_values(['Dataset', 'Avg_Metric'], ascending=[True, False])\n",
    "    print(lgd_results.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ LGD Summary Statistics:\")\n",
    "    print(f\"  Best average metric: {lgd_results['Avg_Metric'].max():.4f}\")\n",
    "    print(f\"  Worst average metric: {lgd_results['Avg_Metric'].min():.4f}\")\n",
    "    print(f\"  Mean across all: {lgd_results['Avg_Metric'].mean():.4f}\")\n",
    "\n",
    "# Save to CSV\n",
    "output_path = PROJECT_ROOT / \"results\" / f\"benchmark_results_{SEED}.csv\"\n",
    "output_path.parent.mkdir(exist_ok=True)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"\\nðŸ’¾ Results saved to: {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" BENCHMARK COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
